{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping blog articles for **Migrants' Rights Network**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 3...\n",
      "Saved article: New migration measures reinforce classism and racism\n",
      "Scraping page 4...\n",
      "Saved article: Digital Hostile Environment: Passports and Facial Recognition\n",
      "Saved article: Deprivation of citizenship is Islamophobic\n",
      "Saved article: MAP: Third Workshop\n",
      "Saved article: Digitisation of the UK border: EVisas\n",
      "Saved article: Data-sharing and immigration enforcement\n",
      "Saved article: Suella’s horrendous legacy: her worst moments\n",
      "Saved article: Digitisation of the UK border: Electronic Travel Authorisation (ETA)\n",
      "Saved article: International Day Against Fascism + Antisemitism\n",
      "Saved article: Silent genocides: Congo, Armenia + Sudan\n",
      "Saved article: Desensitisation to the Global Majority’s Suffering\n",
      "Scraping page 5...\n",
      "Saved article: Islamophobia Awareness Month 2023\n",
      "Saved article: Blog: Bibby Stockholm\n",
      "Saved article: “We are pioneers and innovators”.\n",
      "Saved article: “We made ourselves strong”.\n",
      "Saved article: “Celebrating our Blackness in its entirety”.\n",
      "Saved article: Right to Work Checks + Immigration Raids\n",
      "Saved article: MAP: Second Workshop\n",
      "Saved article: Stop Appeasing Your Racist Uncle\n",
      "Saved article: World Mental Health Day\n",
      "Saved article: Black History Month 2023\n",
      "Scraping page 6...\n",
      "Saved article: Identity Trouble: how stereotypes inform Chinese immigrant identification\n",
      "Saved article: “Our heritage is dynamic + constantly changing”\n",
      "Saved article: Open Letter rejecting the Home Secretary’s abhorrent comments\n",
      "Saved article: Everyone has the right to protection\n",
      "Saved article: MAP: First Workshop\n",
      "Saved article: Migrants’ Aspiration Programme for Hongkongers\n",
      "Saved article: Statement: seven day notice eviction period\n",
      "Saved article: Abolition, not reform\n",
      "Saved article: International Day of Charity\n",
      "Saved article: East + South East Asian Heritage Month\n",
      "Scraping page 7...\n",
      "Saved article: Let’s talk about (Brown) sex, baby\n",
      "Saved article: Rebelling against the colonisers\n",
      "Saved article: Open letters: Migrants’ rights organisations call for Government and Labour to abandon support for cruel asylum accommodation\n",
      "Saved article: No to inhumane accommodation- Sign the open letter to shadow cabinet\n",
      "Saved article: No to inhumane accommodation- Sign the open letter to Government\n",
      "Saved article: The Hostile Environment just got worse.\n",
      "Saved article: Stand With Trans\n",
      "Saved article: Statement: citizenship deprivation + the “good character” requirement\n",
      "Saved article: Inhumane Migration Bill: Open Letter\n",
      "Saved article: Trade Unions + Migrant Organisations’ Statement\n",
      "Scraping page 8...\n",
      "Saved article: South Asian Heritage Month\n",
      "Saved article: The inhumane and cruel Migration Bill is set to become law.\n",
      "Saved article: The gender binary is white supremacy\n",
      "Saved article: Disability Pride Month\n",
      "Saved article: How capitalism harms migrants + queer people\n",
      "Saved article: State oppression of queer people\n",
      "Saved article: Biphobia in the UK asylum system\n",
      "Saved article: Why Cypriots should stand in solidarity with refugees\n",
      "Saved article: Scapegoating of migrants, Muslims + queer people: an intersectional perspective\n",
      "Saved article: World Refugee Day\n",
      "Scraping page 9...\n",
      "Saved article: What changes have there been to right to work guidance for employers?\n",
      "Saved article: Survey: Right to Work Checks\n",
      "Saved article: Pride must continue its revolt\n",
      "Saved article: Migration Figures: Statement\n",
      "Saved article: National Conservatism Conference\n",
      "Saved article: Asylum accommodation: what’s going on?\n",
      "Saved article: Multiculturalism and inclusion are not things to be feared\n",
      "Saved article: Immigration Acts 2014 + 2016\n",
      "Saved article: Workers Memorial Day\n",
      "Saved article: We reject the Immigration Minister’s inflammatory remarks.\n",
      "Scraping page 10...\n",
      "Saved article: The Hostile Environment goes digital\n",
      "Saved article: The Data Protection and Digital Information Bill harms migrants’ rights\n",
      "Saved article: Statement on inappropriate refugee accommodation\n",
      "Saved article: RSHE Open Letter: Signatory\n",
      "Saved article: International Day for Elimination of Racial Discrimination\n",
      "Saved article: Commonwealth Day: Our Statement\n",
      "Saved article: What’s going on with the UK’s immigration laws?\n",
      "Saved article: It’s racist to…\n",
      "Saved article: Britain is an expert in homonationalistic ideology\n",
      "Saved article: How Britain exported homophobia\n",
      "Scraping page 11...\n",
      "Saved article: Shamima Begum: Our Statement\n",
      "Saved article: Letter to HMRC and the Home Office on alleged tax discrepancies: Nadhim Zahawi and migrants\n",
      "Saved article: Queerness and Migration\n",
      "Saved article: Joint Letter on Islamophobia, Knowsley and the Far Right\n",
      "Saved article: Cypriot Queerness Beyond Sexuality\n",
      "Saved article: ‘Inclusion’ celebrates who we are\n",
      "Saved article: The burden of explaining where you are from\n",
      "Saved article: Migration discourse is founded on colonial lies\n",
      "Saved article: A story about Akrotiri + Dhekelia\n",
      "Saved article: There are layers to our struggle, and they are all interconnected\n",
      "Scraping page 12...\n",
      "Saved article: Letter to PM on Missing Children\n",
      "Saved article: Letter to the Immigration Minister\n",
      "Saved article: Albania: MRN + JCWI Joint Statement\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date from of 'month date, year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'[a-zA-Z]+ \\d{1,2}, \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%B %d, %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text and publication date from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None, \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # target the div containing the article text\n",
    "    article_text = soup.find('div', class_='entry-content clear')\n",
    "    \n",
    "    if article_text:\n",
    "        # extract the text, using 'separator' to add spaces where tags are removed\n",
    "        article_text = article_text.get_text(separator=' ', strip=True)\n",
    "    else:\n",
    "        article_text = \"Could not find the article text.\"\n",
    "        print(f\"Could not extract text for: {url}\")\n",
    "    \n",
    "    # separately finding and extracting the article publication date from the 'entry-meta' div\n",
    "    date_container = soup.find('div', class_='entry-meta')\n",
    "    article_date_text = date_container.get_text(strip=True) if date_container else None\n",
    "    article_date = find_date_in_text(article_date_text) if article_date_text else None\n",
    "\n",
    "    return article_date, article_text\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=3, end_page=12):\n",
    "    \"\"\"Crawl the articles from page 3 to page 12 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Organisation', 'Title', 'Date', 'Link', 'Text'])  # Header row\n",
    "\n",
    "        for page in range(start_page, end_page + 1):  # Loop from page 3 to page 12\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            time.sleep(1)  # Wait for 1 second before making each request to be polite\n",
    "            url = f\"{base_url}{page}/\"  # Append the page number to the base URL\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {url}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # 'entry-title ast-blog-single-element' is the class for article titles\n",
    "            article_links = soup.findAll('h2', {'class': 'entry-title ast-blog-single-element'})\n",
    "\n",
    "            for link in article_links:\n",
    "                time.sleep(1)  # Polite delay between requests\n",
    "                article_url = link.find('a')['href']\n",
    "                article_date, article_text = scrape_article(article_url)\n",
    "                \n",
    "                if article_date and start_date <= article_date <= end_date:\n",
    "                    writer.writerow([\"Migrants' Rights Network\", link.text.strip(), article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                    print(f\"Saved article: {link.text.strip()}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://migrantsrights.org.uk/category/blog/page/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping blog articles for **Freedom from Torture**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 0...\n",
      "Saved article: Supreme Court rules plan to send refugees to Rwanda ‘unlawful’\n",
      "Saved article: Freedom from Torture’s statement on Israel and the Occupied Palestinian Territories\n",
      "Saved article: Bibby Stockholm: Why refugees and torture survivors shouldn’t be housed on floating prisons\n",
      "Saved article: My heart aches for young women imprisoned and suffering in Iran today\n",
      "Saved article: Sunak’s heartless proposal to force refugees to live on barges is a mental and physical health catastrophe waiting to happen\n",
      "Saved article: 'Illegal Migration' Act - Everything you need to know\n",
      "Saved article: Refugee Ban Bill will effectively extinguish the right to seek asylum in the UK\n",
      "Saved article: Where does torture happen around the world?\n",
      "Saved article: What is torture?\n",
      "Saved article: Plan to send refugees to Rwanda ‘unlawful’ – A vital win as the Court of Appeal rules on the Government’s plan\n",
      "Saved article: Banned: A peaceful protest to stand up for refugees\n",
      "Saved article: Freedom from Torture wins top prize at the Charity Awards 2023\n",
      "Saved article: Bridgerton star Adjoa Andoh presents Freedom from Torture’s BBC Radio 4 Appeal\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date from of 'date month year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'\\d{1,2} [a-zA-Z]+ \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%d %B %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text and publication date from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return None, \"\", \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article_content = soup.find('div', {'class': 'last-unspace'})\n",
    "    article_text = article_content.get_text(separator=' ', strip=True) if article_content else \"Could not find the article text.\"\n",
    "    \n",
    "    date_container = soup.find('div', class_='field--field-published-date--item')\n",
    "    article_date_text = date_container.get_text(strip=True) if date_container else None\n",
    "    article_date = find_date_in_text(article_date_text) if article_date_text else None\n",
    "\n",
    "    return article_date, article_text\n",
    "\n",
    "def strip_date_from_title(title_with_date):\n",
    "    \"\"\"Delete the date at the start of the title and any following newlines/spaces.\"\"\"\n",
    "    pattern = re.compile(r'^\\d{1,2} [a-zA-Z]+ \\d{4}\\s*[\\n\\r\\s]*')\n",
    "    # Replace the matched date and following whitespace/newlines with an empty string\n",
    "    title_without_date = re.sub(pattern, '', title_with_date).strip()\n",
    "    return title_without_date\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=0, end_page=0):\n",
    "    \"\"\"Crawl the articles on page0 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "        \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            time.sleep(1)\n",
    "            page_url = f\"{base_url}?page={page}\" \n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {page_url}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            article_links = soup.findAll('div', {'class': 'mb-4'})\n",
    "\n",
    "            for link in article_links:\n",
    "                time.sleep(1)  # Polite delay between requests\n",
    "                a_tag = link.find('a')\n",
    "                if a_tag and a_tag['href']:\n",
    "                    article_url = a_tag['href']\n",
    "                    # Check if the URL is relative and prepend the base URL if necessary\n",
    "                    if article_url.startswith('/'):\n",
    "                        article_url = f\"https://www.freedomfromtorture.org{article_url}\"\n",
    "                    article_date, article_text = scrape_article(article_url)\n",
    "                                \n",
    "                    if article_date and start_date <= article_date <= end_date:\n",
    "                        article_title_with_date = link.text.strip()  # Original text containing both title and date\n",
    "                        article_title = strip_date_from_title(article_title_with_date)  # Stripped title\n",
    "                        writer.writerow([\"Freedom from Torture\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                        print(f\"Saved article: {article_title}\")\n",
    "\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.freedomfromtorture.org/news'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping blog articles for **Rainbow Migration**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Stop the Rwanda Bill!\n",
      "Saved article: “I will have to hide my identity in my own room”\n",
      "Saved article: Apply for a trainee solicitor position at Wilson’s\n",
      "Saved article: Joint civil society statement on the Supreme Court ruling on the Rwanda Plan\n",
      "Saved article: Joint Statement: LGBTQI+ people seeking safety here will not be sent to Rwanda\n",
      "Saved article: We are hiring: Legal and Support Services Assistant\n",
      "Saved article: A video guide to intersex asylum claims\n",
      "Saved article: LGBTQI+ people shouldn’t be moved to a floating prison\n",
      "Saved article: Enough: Trans people are people too\n",
      "Saved article: Letter to the PM: Respect the lives of LGBTQI+ people and women seeking asylum in the UK\n",
      "Saved article: Our response to the Home Secretary who thinks “being gay isn’t reason enough for asylum”\n",
      "Saved article: “To have our human rights respected and protected has changed so many lives forever.”\n",
      "Saved article: Brook House Inquiry findings: A gay man faced verbal homophobic abuse and was outed by staff in detention\n",
      "Saved article: “It’s like comparing night and day”: community-based alternatives to detention\n",
      "Saved article: More Albanian LGB+ people granted asylum despite country considered ‘safe‘ under new Illegal Migration Act\n",
      "Saved article: Celebrating 30 years together!\n",
      "Saved article: “I am proud of the work we did together”\n",
      "Saved article: “Rainbow Migration has a lot to celebrate!”\n",
      "Saved article: We are celebrating our 30th anniversary: “I love being part of it”\n",
      "Saved article: Tell your MP about alternatives to detention\n",
      "Saved article: Our 30th anniversary fundraising challenge is here\n",
      "Saved article: LGBTQI+ people seeking asylum will not be safe on barges\n",
      "Saved article: Thank you for speaking out\n",
      "Saved article: How the UK government is turning a blind eye to the discrimination that LGBTQ+ people seeking asylum will face if sent to Rwanda\n",
      "Saved article: MPs raise concerns about this government’s plans to detain and send LGBTQI+ people to Rwanda\n",
      "Saved article: Pride: The right to be safe and to be yourself\n",
      "Saved article: 4 ways in which you can support LGBTQI+ people seeking asylum\n",
      "Saved article: Understanding immigration detention – part 2\n",
      "Saved article: We are hiring: Legal and Policy Director\n",
      "Saved article: New campaign to stop the Refugee Ban Bill\n",
      "Saved article: Understanding immigration detention\n",
      "Saved article: Help direct the future of Rainbow Migration\n",
      "Saved article: We are recruiting people to join our board of trustees\n",
      "Saved article: Why Trans Day of Visibility is so important\n",
      "Saved article: What does a Support Worker at Rainbow Migration do?\n",
      "Saved article: Full breakdown of countries that the asylum bill considers ‘safe’ but are not safe for LGBTQI+ people\n",
      "Saved article: Government’s new asylum bill fails to consider the safety of LGBTQI+ people seeking safety here\n",
      "Saved article: You are invited to our film screening in London\n",
      "Saved article: Podcast about No Pride in Detention\n",
      "Saved article: LGBTQI+ people still at serious risk of harm in immigration detention\n",
      "Saved article: We are hiring: LGBTQI+ Asylum Seeker Support Worker\n",
      "Saved article: Join our campaigns advisory group!\n",
      "Saved article: “It is a way of getting my voice heard in cases where it normally wouldn’t be heard”\n",
      "Saved article: Travelling as a person with refugee status\n",
      "Saved article: Less scrutiny means more risk for LGBTQI+ people in detention\n",
      "Saved article: Survey: Our new vision\n",
      "Saved article: Video: 2022 in review\n",
      "Saved article: Key highlights from 2022\n",
      "Saved article: “I aspire to create a community that can champion kindness and caring”\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date from of 'date/month/year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'[a-zA-Z]+ \\d{1,2}, \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%d/%m/%Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    \"\"\"Investigate different tags and classes to find the main article text.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\", \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article_text = \"\"\n",
    "    \n",
    "    # Check for 'post-content style-light double-bottom-padding' container\n",
    "    main_content = soup.find('div', class_='post-content style-light double-bottom-padding')\n",
    "    if main_content:\n",
    "        # Extract all text from <p> and <h3> tags within this container\n",
    "        for segment in main_content.find_all(['p', 'h3']):\n",
    "            article_text += segment.get_text(strip=True) + \" \"\n",
    "    \n",
    "    # Check for 'uncode_text_column' containers\n",
    "    uncode_columns = soup.findAll('div', class_='uncode_text_column')\n",
    "    for column in uncode_columns:\n",
    "        # Extract all text from <p> and <h3> tags within each 'uncode_text_column' container\n",
    "        for segment in column.find_all(['p', 'h3']):\n",
    "            article_text += segment.get_text(strip=True) + \" \"\n",
    "    \n",
    "    return article_text.strip()\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=3, end_page=11):\n",
    "    \"\"\"Crawl the articles between page3 and page11 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "            \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Loop through the pages from start_page to end_page\n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            page_url = f\"{base_url}?upage={page_num}\"\n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {page_url}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_links = soup.findAll('div', {'class': 't-entry'})\n",
    "\n",
    "            # Loop through the article links on the page\n",
    "            for link in article_links:\n",
    "                time.sleep(1)\n",
    "                a_tag = link.find('a')\n",
    "                if not a_tag or 'href' not in a_tag.attrs:\n",
    "                    continue\n",
    "                article_url = a_tag['href']\n",
    "                article_text = scrape_article(article_url)\n",
    "                # Extract the date from the 't-entry-date' span if it exists\n",
    "                date_span = link.find('span', class_='t-entry-date')\n",
    "                if date_span:\n",
    "                    date_text = date_span.text\n",
    "                    article_date = find_date_in_text(date_text)\n",
    "                else:\n",
    "                    article_date = None\n",
    "\n",
    "                if article_date and start_date <= article_date <= end_date:\n",
    "                    article_title = a_tag.text.strip() \n",
    "                    writer.writerow([\"Rainbow Migration\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                    print(f\"Saved article: {article_title}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.rainbowmigration.org.uk/news/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping *latest news* for **Women for Refugee Women**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: The Supreme Court ruled the Rwanda Plan unlawful\n",
      "Saved article: Suella Braverman’s speech: How it harms women and LGBTQ+ people\n",
      "Saved article: A huge congratulations to Agnes and Loraine for winning the Pioneer 20 award!\n",
      "Saved article: Passage of the ‘Illegal’ Migration Act\n",
      "Saved article: Great news! The 72-hour time limit on the detention of pregnant women is maintained\n",
      "Saved article: Putting Ourselves in the Picture: Rainbow Sisters Virtual Gallery!\n",
      "Saved article: Campaign win! All legal advice surgeries in immigration detention must now take place face-to-face.\n",
      "Saved article: Updated May 2023 – Joint briefing on the ‘Illegal Migration Bill’: Take action against the proposed new powers to detain pregnant women indefinitely\n",
      "Saved article: We are recruiting a Campaigns and Advocacy Manager!\n",
      "Saved article: See Us, Believe Us, Stand With Us\n",
      "Saved article: Our Year: 2022\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date from of 'month date, year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'[a-zA-Z]+ \\d{1,2}, \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%B %d, %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the container div for the article text\n",
    "    article_content = soup.find('div', class_='post-content style-light double-bottom-padding')\n",
    "    if not article_content:\n",
    "        print(\"Article content container not found.\")\n",
    "        return \"Could not find the article text.\"\n",
    "    \n",
    "    # Extract and return all text within the container\n",
    "    article_text = article_content.get_text(separator=\" \", strip=True)\n",
    "    return article_text\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename):\n",
    "    \"\"\"Crawl the articles between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "            \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_links = soup.findAll('div', {'class': 't-entry'})\n",
    "\n",
    "        for link in article_links:\n",
    "            time.sleep(1)\n",
    "            a_tag = link.find('a')\n",
    "            if not a_tag or 'href' not in a_tag.attrs:\n",
    "                continue\n",
    "            article_url = a_tag['href']\n",
    "            article_text = scrape_article(article_url)\n",
    "            # Extract the date from the 't-entry-date' span if it exists\n",
    "            date_span = link.find('span', class_='t-entry-date')\n",
    "            if date_span:\n",
    "                date_text = date_span.text\n",
    "                article_date = find_date_in_text(date_text)\n",
    "            else:\n",
    "                article_date = None\n",
    "\n",
    "            if article_date and start_date <= article_date <= end_date:\n",
    "                article_title = a_tag.text.strip()\n",
    "                writer.writerow([\"Women for Refugee Women\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                print(f\"Saved article: {article_title}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.refugeewomen.co.uk/news/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping *blog articles* for **Women for Refugee Women**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article content container not found.\n",
      "Saved article: Welcome Every Woman: A Festive Celebration\n",
      "Saved article: Celebrating this year’s empowerment activities\n",
      "Saved article: Hiba’s Story: Pride 2023\n",
      "Saved article: Ange’s Story: Pride 2023\n",
      "Saved article: A Collaboration between Women for Refugee Women and The Five Points Brewing Co.\n",
      "Saved article: Guest blog: Our Mothers Ourselves\n",
      "Saved article: We held our first in-person Christmas party since 2019\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date from of 'month date, year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'[a-zA-Z]+ \\d{1,2}, \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%B %d, %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the container div for the article text\n",
    "    article_content = soup.find('div', class_='post-content style-light double-bottom-padding')\n",
    "    if not article_content:\n",
    "        print(\"Article content container not found.\")\n",
    "        return \"Could not find the article text.\"\n",
    "    \n",
    "    # Extract and return all text within the container. This includes text within <span> tags.\n",
    "    article_text = article_content.get_text(separator=\" \", strip=True)\n",
    "    return article_text\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename):\n",
    "    \"\"\"Crawl the articles between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "            \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_links = soup.findAll('div', {'class': 't-entry'})\n",
    "\n",
    "        for link in article_links:\n",
    "            time.sleep(1)\n",
    "            a_tag = link.find('a')\n",
    "            if not a_tag or 'href' not in a_tag.attrs:\n",
    "                continue\n",
    "            article_url = a_tag['href']\n",
    "            article_text = scrape_article(article_url)\n",
    "            # Extract the date from the 't-entry-date' span if it exists\n",
    "            date_span = link.find('span', class_='t-entry-date')\n",
    "            if date_span:\n",
    "                date_text = date_span.text\n",
    "                article_date = find_date_in_text(date_text)\n",
    "            else:\n",
    "                article_date = None\n",
    "\n",
    "            if article_date and start_date <= article_date <= end_date:\n",
    "                article_title = a_tag.text.strip()\n",
    "                writer.writerow([\"Women for Refugee Women\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                print(f\"Saved article: {article_title}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.refugeewomen.co.uk/news/blog/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
