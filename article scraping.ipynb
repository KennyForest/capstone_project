{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Migrants' Rights Network**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 3...\n",
      "Saved article: New migration measures reinforce classism and racism\n",
      "Scraping page 4...\n",
      "Saved article: Digital Hostile Environment: Passports and Facial Recognition\n",
      "Saved article: Deprivation of citizenship is Islamophobic\n",
      "Saved article: MAP: Third Workshop\n",
      "Saved article: Digitisation of the UK border: EVisas\n",
      "Saved article: Data-sharing and immigration enforcement\n",
      "Saved article: Suella’s horrendous legacy: her worst moments\n",
      "Saved article: Digitisation of the UK border: Electronic Travel Authorisation (ETA)\n",
      "Saved article: International Day Against Fascism + Antisemitism\n",
      "Saved article: Silent genocides: Congo, Armenia + Sudan\n",
      "Saved article: Desensitisation to the Global Majority’s Suffering\n",
      "Scraping page 5...\n",
      "Saved article: Islamophobia Awareness Month 2023\n",
      "Saved article: Blog: Bibby Stockholm\n",
      "Saved article: “We are pioneers and innovators”.\n",
      "Saved article: “We made ourselves strong”.\n",
      "Saved article: “Celebrating our Blackness in its entirety”.\n",
      "Saved article: Right to Work Checks + Immigration Raids\n",
      "Saved article: MAP: Second Workshop\n",
      "Saved article: Stop Appeasing Your Racist Uncle\n",
      "Saved article: World Mental Health Day\n",
      "Saved article: Black History Month 2023\n",
      "Scraping page 6...\n",
      "Saved article: Identity Trouble: how stereotypes inform Chinese immigrant identification\n",
      "Saved article: “Our heritage is dynamic + constantly changing”\n",
      "Saved article: Open Letter rejecting the Home Secretary’s abhorrent comments\n",
      "Saved article: Everyone has the right to protection\n",
      "Saved article: MAP: First Workshop\n",
      "Saved article: Migrants’ Aspiration Programme for Hongkongers\n",
      "Saved article: Statement: seven day notice eviction period\n",
      "Saved article: Abolition, not reform\n",
      "Saved article: International Day of Charity\n",
      "Saved article: East + South East Asian Heritage Month\n",
      "Scraping page 7...\n",
      "Saved article: Let’s talk about (Brown) sex, baby\n",
      "Saved article: Rebelling against the colonisers\n",
      "Saved article: Open letters: Migrants’ rights organisations call for Government and Labour to abandon support for cruel asylum accommodation\n",
      "Saved article: No to inhumane accommodation- Sign the open letter to shadow cabinet\n",
      "Saved article: No to inhumane accommodation- Sign the open letter to Government\n",
      "Saved article: The Hostile Environment just got worse.\n",
      "Saved article: Stand With Trans\n",
      "Saved article: Statement: citizenship deprivation + the “good character” requirement\n",
      "Saved article: Inhumane Migration Bill: Open Letter\n",
      "Saved article: Trade Unions + Migrant Organisations’ Statement\n",
      "Scraping page 8...\n",
      "Saved article: South Asian Heritage Month\n",
      "Saved article: The inhumane and cruel Migration Bill is set to become law.\n",
      "Saved article: The gender binary is white supremacy\n",
      "Saved article: Disability Pride Month\n",
      "Saved article: How capitalism harms migrants + queer people\n",
      "Saved article: State oppression of queer people\n",
      "Saved article: Biphobia in the UK asylum system\n",
      "Saved article: Why Cypriots should stand in solidarity with refugees\n",
      "Saved article: Scapegoating of migrants, Muslims + queer people: an intersectional perspective\n",
      "Saved article: World Refugee Day\n",
      "Scraping page 9...\n",
      "Saved article: What changes have there been to right to work guidance for employers?\n",
      "Saved article: Survey: Right to Work Checks\n",
      "Saved article: Pride must continue its revolt\n",
      "Saved article: Migration Figures: Statement\n",
      "Saved article: National Conservatism Conference\n",
      "Saved article: Asylum accommodation: what’s going on?\n",
      "Saved article: Multiculturalism and inclusion are not things to be feared\n",
      "Saved article: Immigration Acts 2014 + 2016\n",
      "Saved article: Workers Memorial Day\n",
      "Saved article: We reject the Immigration Minister’s inflammatory remarks.\n",
      "Scraping page 10...\n",
      "Saved article: The Hostile Environment goes digital\n",
      "Saved article: The Data Protection and Digital Information Bill harms migrants’ rights\n",
      "Saved article: Statement on inappropriate refugee accommodation\n",
      "Saved article: RSHE Open Letter: Signatory\n",
      "Saved article: International Day for Elimination of Racial Discrimination\n",
      "Saved article: Commonwealth Day: Our Statement\n",
      "Saved article: What’s going on with the UK’s immigration laws?\n",
      "Saved article: It’s racist to…\n",
      "Saved article: Britain is an expert in homonationalistic ideology\n",
      "Saved article: How Britain exported homophobia\n",
      "Scraping page 11...\n",
      "Saved article: Shamima Begum: Our Statement\n",
      "Saved article: Letter to HMRC and the Home Office on alleged tax discrepancies: Nadhim Zahawi and migrants\n",
      "Saved article: Queerness and Migration\n",
      "Saved article: Joint Letter on Islamophobia, Knowsley and the Far Right\n",
      "Saved article: Cypriot Queerness Beyond Sexuality\n",
      "Saved article: ‘Inclusion’ celebrates who we are\n",
      "Saved article: The burden of explaining where you are from\n",
      "Saved article: Migration discourse is founded on colonial lies\n",
      "Saved article: A story about Akrotiri + Dhekelia\n",
      "Saved article: There are layers to our struggle, and they are all interconnected\n",
      "Scraping page 12...\n",
      "Saved article: Letter to PM on Missing Children\n",
      "Saved article: Letter to the Immigration Minister\n",
      "Saved article: Albania: MRN + JCWI Joint Statement\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date form of 'month date, year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'[a-zA-Z]+ \\d{1,2}, \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%B %d, %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text and publication date from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return None, \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # target the div containing the article text\n",
    "    article_text = soup.find('div', class_='entry-content clear')\n",
    "    \n",
    "    if article_text:\n",
    "        # extract the text, using 'separator' to add spaces where tags are removed\n",
    "        article_text = article_text.get_text(separator=' ', strip=True)\n",
    "    else:\n",
    "        article_text = \"Could not find the article text.\"\n",
    "        print(f\"Could not extract text for: {url}\")\n",
    "    \n",
    "    # separately finding and extracting the article publication date from the 'entry-meta' div\n",
    "    date_container = soup.find('div', class_='entry-meta')\n",
    "    article_date_text = date_container.get_text(strip=True) if date_container else None\n",
    "    article_date = find_date_in_text(article_date_text) if article_date_text else None\n",
    "\n",
    "    return article_date, article_text\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=3, end_page=12):\n",
    "    \"\"\"Crawl the articles from page 3 to page 12 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Organisation', 'Title', 'Date', 'Link', 'Text'])  # Header row\n",
    "\n",
    "        for page in range(start_page, end_page + 1):  # Loop from page 3 to page 12\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            time.sleep(1)  # Wait for 1 second before making each request to be polite\n",
    "            url = f\"{base_url}{page}/\"  # Append the page number to the base URL\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {url}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # 'entry-title ast-blog-single-element' is the class for article titles\n",
    "            article_links = soup.findAll('h2', {'class': 'entry-title ast-blog-single-element'})\n",
    "\n",
    "            for link in article_links:\n",
    "                time.sleep(1)  # Polite delay between requests\n",
    "                article_url = link.find('a')['href']\n",
    "                article_date, article_text = scrape_article(article_url)\n",
    "                \n",
    "                if article_date and start_date <= article_date <= end_date:\n",
    "                    writer.writerow([\"Migrants' Rights Network\", link.text.strip(), article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                    print(f\"Saved article: {link.text.strip()}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://migrantsrights.org.uk/category/blog/page/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Freedom from Torture**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 0...\n",
      "Saved article: Supreme Court rules plan to send refugees to Rwanda ‘unlawful’\n",
      "Saved article: Freedom from Torture’s statement on Israel and the Occupied Palestinian Territories\n",
      "Saved article: Bibby Stockholm: Why refugees and torture survivors shouldn’t be housed on floating prisons\n",
      "Saved article: My heart aches for young women imprisoned and suffering in Iran today\n",
      "Saved article: Sunak’s heartless proposal to force refugees to live on barges is a mental and physical health catastrophe waiting to happen\n",
      "Saved article: 'Illegal Migration' Act - Everything you need to know\n",
      "Saved article: Refugee Ban Bill will effectively extinguish the right to seek asylum in the UK\n",
      "Saved article: Where does torture happen around the world?\n",
      "Saved article: What is torture?\n",
      "Saved article: Plan to send refugees to Rwanda ‘unlawful’ – A vital win as the Court of Appeal rules on the Government’s plan\n",
      "Saved article: Banned: A peaceful protest to stand up for refugees\n",
      "Saved article: Freedom from Torture wins top prize at the Charity Awards 2023\n",
      "Saved article: Bridgerton star Adjoa Andoh presents Freedom from Torture’s BBC Radio 4 Appeal\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date form of 'date month year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'\\d{1,2} [a-zA-Z]+ \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%d %B %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text and publication date from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return None, \"\", \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article_content = soup.find('div', {'class': 'last-unspace'})\n",
    "    article_text = article_content.get_text(separator=' ', strip=True) if article_content else \"Could not find the article text.\"\n",
    "    \n",
    "    date_container = soup.find('div', class_='field--field-published-date--item')\n",
    "    article_date_text = date_container.get_text(strip=True) if date_container else None\n",
    "    article_date = find_date_in_text(article_date_text) if article_date_text else None\n",
    "\n",
    "    return article_date, article_text\n",
    "\n",
    "def strip_date_from_title(title_with_date):\n",
    "    \"\"\"Delete the date at the start of the title and any following newlines/spaces.\"\"\"\n",
    "    pattern = re.compile(r'^\\d{1,2} [a-zA-Z]+ \\d{4}\\s*[\\n\\r\\s]*')\n",
    "    # Replace the matched date and following whitespace/newlines with an empty string\n",
    "    title_without_date = re.sub(pattern, '', title_with_date).strip()\n",
    "    return title_without_date\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=0, end_page=0):\n",
    "    \"\"\"Crawl the articles on page0 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "        \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            time.sleep(1)\n",
    "            page_url = f\"{base_url}?page={page}\" \n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {page_url}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            article_links = soup.findAll('div', {'class': 'mb-4'})\n",
    "\n",
    "            for link in article_links:\n",
    "                time.sleep(1)  # Polite delay between requests\n",
    "                a_tag = link.find('a')\n",
    "                if a_tag and a_tag['href']:\n",
    "                    article_url = a_tag['href']\n",
    "                    # Check if the URL is relative and prepend the base URL if necessary\n",
    "                    if article_url.startswith('/'):\n",
    "                        article_url = f\"https://www.freedomfromtorture.org{article_url}\"\n",
    "                    article_date, article_text = scrape_article(article_url)\n",
    "                                \n",
    "                    if article_date and start_date <= article_date <= end_date:\n",
    "                        article_title_with_date = link.text.strip()  # Original text containing both title and date\n",
    "                        article_title = strip_date_from_title(article_title_with_date)  # Stripped title\n",
    "                        writer.writerow([\"Freedom from Torture\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                        print(f\"Saved article: {article_title}\")\n",
    "\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.freedomfromtorture.org/news'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Rainbow Migration**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Stop the Rwanda Bill!\n",
      "Saved article: “I will have to hide my identity in my own room”\n",
      "Saved article: Apply for a trainee solicitor position at Wilson’s\n",
      "Saved article: Joint civil society statement on the Supreme Court ruling on the Rwanda Plan\n",
      "Saved article: Joint Statement: LGBTQI+ people seeking safety here will not be sent to Rwanda\n",
      "Saved article: We are hiring: Legal and Support Services Assistant\n",
      "Saved article: A video guide to intersex asylum claims\n",
      "Saved article: LGBTQI+ people shouldn’t be moved to a floating prison\n",
      "Saved article: Enough: Trans people are people too\n",
      "Saved article: Letter to the PM: Respect the lives of LGBTQI+ people and women seeking asylum in the UK\n",
      "Saved article: Our response to the Home Secretary who thinks “being gay isn’t reason enough for asylum”\n",
      "Saved article: “To have our human rights respected and protected has changed so many lives forever.”\n",
      "Saved article: Brook House Inquiry findings: A gay man faced verbal homophobic abuse and was outed by staff in detention\n",
      "Saved article: “It’s like comparing night and day”: community-based alternatives to detention\n",
      "Saved article: More Albanian LGB+ people granted asylum despite country considered ‘safe‘ under new Illegal Migration Act\n",
      "Saved article: Celebrating 30 years together!\n",
      "Saved article: “I am proud of the work we did together”\n",
      "Saved article: “Rainbow Migration has a lot to celebrate!”\n",
      "Saved article: We are celebrating our 30th anniversary: “I love being part of it”\n",
      "Saved article: Tell your MP about alternatives to detention\n",
      "Saved article: Our 30th anniversary fundraising challenge is here\n",
      "Saved article: LGBTQI+ people seeking asylum will not be safe on barges\n",
      "Saved article: Thank you for speaking out\n",
      "Saved article: How the UK government is turning a blind eye to the discrimination that LGBTQ+ people seeking asylum will face if sent to Rwanda\n",
      "Saved article: MPs raise concerns about this government’s plans to detain and send LGBTQI+ people to Rwanda\n",
      "Saved article: Pride: The right to be safe and to be yourself\n",
      "Saved article: 4 ways in which you can support LGBTQI+ people seeking asylum\n",
      "Saved article: Understanding immigration detention – part 2\n",
      "Saved article: We are hiring: Legal and Policy Director\n",
      "Saved article: New campaign to stop the Refugee Ban Bill\n",
      "Saved article: Understanding immigration detention\n",
      "Saved article: Help direct the future of Rainbow Migration\n",
      "Saved article: We are recruiting people to join our board of trustees\n",
      "Saved article: Why Trans Day of Visibility is so important\n",
      "Saved article: What does a Support Worker at Rainbow Migration do?\n",
      "Saved article: Full breakdown of countries that the asylum bill considers ‘safe’ but are not safe for LGBTQI+ people\n",
      "Saved article: Government’s new asylum bill fails to consider the safety of LGBTQI+ people seeking safety here\n",
      "Saved article: You are invited to our film screening in London\n",
      "Saved article: Podcast about No Pride in Detention\n",
      "Saved article: LGBTQI+ people still at serious risk of harm in immigration detention\n",
      "Saved article: We are hiring: LGBTQI+ Asylum Seeker Support Worker\n",
      "Saved article: Join our campaigns advisory group!\n",
      "Saved article: “It is a way of getting my voice heard in cases where it normally wouldn’t be heard”\n",
      "Saved article: Travelling as a person with refugee status\n",
      "Saved article: Less scrutiny means more risk for LGBTQI+ people in detention\n",
      "Saved article: Survey: Our new vision\n",
      "Saved article: Video: 2022 in review\n",
      "Saved article: Key highlights from 2022\n",
      "Saved article: “I aspire to create a community that can champion kindness and caring”\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date form of 'date/month/year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'[a-zA-Z]+ \\d{1,2}, \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%d/%m/%Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    \"\"\"Investigate different tags and classes to find the main article text.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\", \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article_text = \"\"\n",
    "    \n",
    "    # Check for 'post-content style-light double-bottom-padding' container\n",
    "    main_content = soup.find('div', class_='post-content style-light double-bottom-padding')\n",
    "    if main_content:\n",
    "        # Extract all text from <p> and <h3> tags within this container\n",
    "        for segment in main_content.find_all(['p', 'h3']):\n",
    "            article_text += segment.get_text(strip=True) + \" \"\n",
    "    \n",
    "    # Check for 'uncode_text_column' containers\n",
    "    uncode_columns = soup.findAll('div', class_='uncode_text_column')\n",
    "    for column in uncode_columns:\n",
    "        # Extract all text from <p> and <h3> tags within each 'uncode_text_column' container\n",
    "        for segment in column.find_all(['p', 'h3']):\n",
    "            article_text += segment.get_text(strip=True) + \" \"\n",
    "    \n",
    "    return article_text.strip()\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=3, end_page=11):\n",
    "    \"\"\"Crawl the articles between page3 and page11 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "            \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        # Loop through the pages from start_page to end_page\n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            page_url = f\"{base_url}?upage={page_num}\"\n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {page_url}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_links = soup.findAll('div', {'class': 't-entry'})\n",
    "\n",
    "            # Loop through the article links on the page\n",
    "            for link in article_links:\n",
    "                time.sleep(1)\n",
    "                a_tag = link.find('a')\n",
    "                if not a_tag or 'href' not in a_tag.attrs:\n",
    "                    continue\n",
    "                article_url = a_tag['href']\n",
    "                article_text = scrape_article(article_url)\n",
    "                # Extract the date from the 't-entry-date' span if it exists\n",
    "                date_span = link.find('span', class_='t-entry-date')\n",
    "                if date_span:\n",
    "                    date_text = date_span.text\n",
    "                    article_date = find_date_in_text(date_text)\n",
    "                else:\n",
    "                    article_date = None\n",
    "\n",
    "                if article_date and start_date <= article_date <= end_date:\n",
    "                    article_title = a_tag.text.strip() \n",
    "                    writer.writerow([\"Rainbow Migration\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                    print(f\"Saved article: {article_title}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.rainbowmigration.org.uk/news/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping *latest news* from **Women for Refugee Women**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: The Supreme Court ruled the Rwanda Plan unlawful\n",
      "Saved article: Suella Braverman’s speech: How it harms women and LGBTQ+ people\n",
      "Saved article: A huge congratulations to Agnes and Loraine for winning the Pioneer 20 award!\n",
      "Saved article: Passage of the ‘Illegal’ Migration Act\n",
      "Saved article: Great news! The 72-hour time limit on the detention of pregnant women is maintained\n",
      "Saved article: Putting Ourselves in the Picture: Rainbow Sisters Virtual Gallery!\n",
      "Saved article: Campaign win! All legal advice surgeries in immigration detention must now take place face-to-face.\n",
      "Saved article: Updated May 2023 – Joint briefing on the ‘Illegal Migration Bill’: Take action against the proposed new powers to detain pregnant women indefinitely\n",
      "Saved article: We are recruiting a Campaigns and Advocacy Manager!\n",
      "Saved article: See Us, Believe Us, Stand With Us\n",
      "Saved article: Our Year: 2022\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date from of 'month date, year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'[a-zA-Z]+ \\d{1,2}, \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%B %d, %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the container div for the article text\n",
    "    article_content = soup.find('div', class_='post-content style-light double-bottom-padding')\n",
    "    if not article_content:\n",
    "        print(\"Article content container not found.\")\n",
    "        return \"Could not find the article text.\"\n",
    "    \n",
    "    # Extract and return all text within the container\n",
    "    article_text = article_content.get_text(separator=\" \", strip=True)\n",
    "    return article_text\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename):\n",
    "    \"\"\"Crawl the articles between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "            \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_links = soup.findAll('div', {'class': 't-entry'})\n",
    "\n",
    "        for link in article_links:\n",
    "            time.sleep(1)\n",
    "            a_tag = link.find('a')\n",
    "            if not a_tag or 'href' not in a_tag.attrs:\n",
    "                continue\n",
    "            article_url = a_tag['href']\n",
    "            article_text = scrape_article(article_url)\n",
    "            # Extract the date from the 't-entry-date' span if it exists\n",
    "            date_span = link.find('span', class_='t-entry-date')\n",
    "            if date_span:\n",
    "                date_text = date_span.text\n",
    "                article_date = find_date_in_text(date_text)\n",
    "            else:\n",
    "                article_date = None\n",
    "\n",
    "            if article_date and start_date <= article_date <= end_date:\n",
    "                article_title = a_tag.text.strip()\n",
    "                writer.writerow([\"Women for Refugee Women\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                print(f\"Saved article: {article_title}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.refugeewomen.co.uk/news/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping *blog articles* from **Women for Refugee Women**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article content container not found.\n",
      "Saved article: Welcome Every Woman: A Festive Celebration\n",
      "Saved article: Celebrating this year’s empowerment activities\n",
      "Saved article: Hiba’s Story: Pride 2023\n",
      "Saved article: Ange’s Story: Pride 2023\n",
      "Saved article: A Collaboration between Women for Refugee Women and The Five Points Brewing Co.\n",
      "Saved article: Guest blog: Our Mothers Ourselves\n",
      "Saved article: We held our first in-person Christmas party since 2019\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date from of 'month date, year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'[a-zA-Z]+ \\d{1,2}, \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%B %d, %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the container div for the article text\n",
    "    article_content = soup.find('div', class_='post-content style-light double-bottom-padding')\n",
    "    if not article_content:\n",
    "        print(\"Article content container not found.\")\n",
    "        return \"Could not find the article text.\"\n",
    "    \n",
    "    # Extract and return all text within the container. This includes text within <span> tags.\n",
    "    article_text = article_content.get_text(separator=\" \", strip=True)\n",
    "    return article_text\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename):\n",
    "    \"\"\"Crawl the articles between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "            \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_links = soup.findAll('div', {'class': 't-entry'})\n",
    "\n",
    "        for link in article_links:\n",
    "            time.sleep(1)\n",
    "            a_tag = link.find('a')\n",
    "            if not a_tag or 'href' not in a_tag.attrs:\n",
    "                continue\n",
    "            article_url = a_tag['href']\n",
    "            article_text = scrape_article(article_url)\n",
    "            # Extract the date from the 't-entry-date' span if it exists\n",
    "            date_span = link.find('span', class_='t-entry-date')\n",
    "            if date_span:\n",
    "                date_text = date_span.text\n",
    "                article_date = find_date_in_text(date_text)\n",
    "            else:\n",
    "                article_date = None\n",
    "\n",
    "            if article_date and start_date <= article_date <= end_date:\n",
    "                article_title = a_tag.text.strip()\n",
    "                writer.writerow([\"Women for Refugee Women\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                print(f\"Saved article: {article_title}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.refugeewomen.co.uk/news/blog/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Young Roots**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Young refugees are at unprecedented risk of homelessness\n",
      "Saved article: Introducing our new Chief Executive, Paola Uccellari\n",
      "Saved article: Young Roots Comedy Night\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date from of 'date/month/year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'\\b\\d{2}/\\d{2}/\\d{4}\\b')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%d/%m/%Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    # Concatenate base URL with the adjusted relative URL\n",
    "    url = base_url.rstrip('/') + url\n",
    "    # Remove any duplicate 'blog' in the URL\n",
    "    url = url.replace('/blog/blog/', '/blog/')\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article_text = \"\"\n",
    "    \n",
    "    # Check for 'post-content style-light double-bottom-padding' container\n",
    "    main_content = soup.find('div', class_='post-content style-light double-bottom-padding')\n",
    "    if main_content:\n",
    "        # Extract all text from <p> and <h3> tags within this container\n",
    "        for segment in main_content.find_all(['p', 'h3']):\n",
    "            article_text += segment.get_text(strip=True) + \" \"\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename):\n",
    "    \"\"\"Crawl the articles between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "            \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_links = soup.findAll('div', {'class': 'blog-basic-grid--text'})\n",
    "\n",
    "        for link in article_links:\n",
    "            time.sleep(1)\n",
    "            a_tag = link.find('a')\n",
    "            if not a_tag or 'href' not in a_tag.attrs:\n",
    "                continue\n",
    "            article_url = a_tag['href']\n",
    "            article_text = scrape_article(article_url)\n",
    "            # Extract the date from the time class if it exists\n",
    "            date_time = link.find('time', class_='blog-date')\n",
    "            if date_time:\n",
    "                date_text = date_time.text\n",
    "                article_date = find_date_in_text(date_text)\n",
    "            else:\n",
    "                article_date = None\n",
    "\n",
    "            if article_date and start_date <= article_date <= end_date:\n",
    "                article_title = a_tag.text.strip()\n",
    "                full_url = '≈' + article_url\n",
    "                writer.writerow([\"Young Roots\", article_title, article_date.strftime('%d %B %Y'), full_url, article_text])\n",
    "                print(f\"Saved article: {article_title}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.youngroots.org.uk/blog'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Sanctuary in Chichester**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: New schools liaison project\n",
      "Saved article: Staff changes\n",
      "Saved article: How Not to Drown – theatre fundraiser\n",
      "Saved article: From Adversity to University\n",
      "Saved article: Drop-in art\n",
      "Saved article: 2023 New Year party\n",
      "Saved article: Give Peace a Chance! – Benefit concert, featuring Ukrainian Volya choir\n",
      "Saved article: New English support sessions for driving tests\n",
      "Saved article: Sculptures and art cathedral exhibition celebrating resilience and values for Refugee Week\n",
      "Saved article: Subjects of sculpture exhibition describe their experience of the project…\n",
      "Saved article: A Culinary Journey: Exploring Syria Through Cuisine\n",
      "Saved article: ‘Resilience in Clay’ Sculpture Subjects Describe Their Experiences at Cathedral Event\n",
      "Saved article: Giving Back: Beneficiaries’ Empowerment through Volunteering with UK Harvest and St Wilfrid’s Hospice\n",
      "Saved article: Celebrating Togetherness at Our Annual Summer Party &\n",
      "Saved article: Report on Our Summer Intensive English Course\n",
      "Saved article: Meet New Members of Sanctuary’s Team: Claudie and James\n",
      "Saved article: Trustee Mike Runs the Great South Run For Sanctuary in Chichester\n",
      "Saved article: Volunteer Training Day\n",
      "Saved article: Update: Sanctuary in Chichester Response to Asylum Hotel\n",
      "Saved article: Sanctuary Selects: A Collaboration with Pallant House Gallery\n",
      "Saved article: Run Complete! Mike Mansergh Completes the Great South Run for Sanctuary\n",
      "Saved article: A Tribute for Joy Constantin\n",
      "Saved article: Latest Update: The Hotel\n",
      "Saved article: Sanctuary in Chichester AGM 2023\n",
      "Saved article: Bertie the Cathedral Cat: A Heartwarming Tail\n",
      "Saved article: ‘Welcome to Britain’: The Colouring Book for Hope\n"
     ]
    }
   ],
   "source": [
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_content = soup.find('div', class_='entry-content')\n",
    "        if article_content:\n",
    "            return article_content.get_text(separator=\" \", strip=True)\n",
    "    print(f\"Error or no content at {url}\")\n",
    "    return None\n",
    "\n",
    "def generate_date_urls(start_date, end_date):\n",
    "    \"\"\"Generate URLs for each day between start_date and end_date.\"\"\"\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        yield f\"https://sanctuaryinchichester.org/{current_date.strftime('%Y/%m/%d/')}\"\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "def find_articles_on_page(url):\n",
    "    \"\"\"Fetch all article links from a given date URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = []\n",
    "        for entry in soup.find_all('h2', class_='entry-title'):\n",
    "            a_tag = entry.find('a', href=True)\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                articles.append((a_tag['href'], a_tag.get_text(strip=True).replace(u'\\xa0', u' ')))  # Replace non-breaking spaces\n",
    "        return articles\n",
    "    return []\n",
    "\n",
    "def extract_date_from_url(url):\n",
    "    \"\"\"Extract the date from the article URL based on format '/YYYY/MM/DD/'.\"\"\"\n",
    "    date_match = re.search(r'/(\\d{4}/\\d{2}/\\d{2})/', url)\n",
    "    if date_match:\n",
    "        return datetime.strptime(date_match.group(1), '%Y/%m/%d').strftime('%d %B %Y')\n",
    "    return \"Unknown Date\"  # Fallback if no date found\n",
    "\n",
    "def crawl_articles(start_date, end_date, csv_filename):\n",
    "    \"\"\"Crawl articles between the start and end dates.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "\n",
    "        writer = csv.writer(file)\n",
    "        for date_url in generate_date_urls(start_date, end_date):\n",
    "            time.sleep(1)  # Delay to respect the server\n",
    "            article_data = find_articles_on_page(date_url)\n",
    "            for article_url, article_title in article_data:\n",
    "                time.sleep(1)  # Respectful delay\n",
    "                article_text = scrape_article(article_url)\n",
    "                if article_text:\n",
    "                    article_date = extract_date_from_url(article_url)\n",
    "                    writer.writerow([\"Sanctuary in Chichester\", article_title, article_date, article_url, article_text])\n",
    "                    print(f\"Saved article: {article_title}\")\n",
    "\n",
    "\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', csv_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **STAR (Student Action for Refugees)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Sign up for an info session to find out more about how to apply to university in the UK as a Refugee/Asylum seeker\n",
      "Saved article: Open up safe routes to the UK for those seeking refuge from the Gaza/ Israel conflict\n",
      "Saved article: No Dad Jokes Without Dad\n",
      "Saved article: Why you should join STAR in 2023-2024\n",
      "Saved article: Good news for refugees wanting to study in the UK\n",
      "Saved article: STAR’s new stickers: A big thank you to FastPrint!\n",
      "Saved article: Statement on the passage of the Illegal Migration Act\n",
      "Saved article: University students and staff oppose the Illegal Migration Bill\n",
      "Saved article: STAR’s AGM and National Meetup 2023!\n",
      "Saved article: Refugee Week\n",
      "Saved article: A 1,000 mile journey for STAR!\n",
      "Saved article: Celebrating STARs this Volunteers Week – Jiayi\n",
      "Saved article: Celebrating STARs this Volunteers Week – Peter\n",
      "Saved article: Celebrating STARs this Volunteers Week – Shehany\n",
      "Saved article: How to keep fighting the Refugee Ban Bill\n",
      "Saved article: Looking for a university scholarship?\n",
      "Saved article: A warm welcome to Warwick Uni!\n",
      "Saved article: This Mother’s Day, call on the government to reunite families\n",
      "Saved article: STAR’s response to the Refugee Ban Bill\n",
      "Saved article: Student Volunteering Week 2023: Leeds bike project\n",
      "Saved article: Together we can stop the flights\n",
      "Saved article: STAR Christmas events: A time for giving\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date form of 'date month year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'\\d{1,2} [a-zA-Z]+ \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%d %B %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text and title from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return None, \"Could not fetch article\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract the title from the <h1> tag\n",
    "    title_tag = soup.find('h1', class_='ast-advanced-headers-title')\n",
    "    article_title = title_tag.get_text(strip=True) if title_tag else \"Title Not Found\"\n",
    "\n",
    "    # Extract the content from the <div> class\n",
    "    article_content = soup.find('div', {'class': 'entry-content'})\n",
    "    article_text = article_content.get_text(separator=' ', strip=True) if article_content else \"Could not find the article text.\"\n",
    "\n",
    "    return article_text, article_title\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=1, end_page=3):\n",
    "    \"\"\"Crawl the articles from page 1 to 3 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "        \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            page_url = f\"{base_url}{page}/\"\n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {page_url}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_links = soup.findAll('div', {'class': 'post-content ast-grid-common-col'})\n",
    "\n",
    "            for link in article_links:\n",
    "                time.sleep(1)\n",
    "                a_tag = link.find('a')\n",
    "                if not a_tag or 'href' not in a_tag.attrs:\n",
    "                    continue\n",
    "                article_url = a_tag['href']\n",
    "                article_text, article_title = scrape_article(article_url)\n",
    "                # Extract the date from the span class if it exists\n",
    "                date_time = link.find('span', class_='published')\n",
    "                if date_time:\n",
    "                    date_text = date_time.text\n",
    "                    article_date = find_date_in_text(date_text)\n",
    "                else:\n",
    "                    article_date = None\n",
    "\n",
    "                if article_date and start_date <= article_date <= end_date:\n",
    "                    writer.writerow([\"STAR\", article_title, article_date, article_url, article_text])\n",
    "                    print(f\"Saved article: {article_title}\")\n",
    "\n",
    "\n",
    "# execution\n",
    "base_url = 'https://star-network.org.uk/category/news/page/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Racial Justice Network**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Exploring Racial Justice: The ‘Rwanda Policy’ Debacle and the Constraints of Our Electoral-Democratic System\n",
      "Saved article: Standing in solidarity with our siblings in Palestine, Sudan, the Democratic Republic of Congo and Yemen, against continued colonisation\n",
      "Saved article: Annual Gather Up Celebration 2023\n",
      "Saved article: Celebrating and Reflecting – International Symposium with Elder Ngũgĩ wa Thiong’o\n",
      "Saved article: Press Release – International Symposium\n",
      "Saved article: URC Reflections: Wealth Distribution\n",
      "Saved article: Stop The Data Discrimination:\n",
      "Saved article: Reflections on Unlearning Racism\n",
      "Saved article: The Bounce Back: Storytelling as a Form of Resilience\n",
      "Saved article: Resisting The Migration Bill\n",
      "Saved article: EVENT: Solidarity With Congo\n",
      "Saved article: Event; Stop The Bill\n",
      "Saved article: The Hola Massacre: the last straw that toppled colonial Kenya\n",
      "Saved article: Celebrating 7 years of RJN\n",
      "Saved article: Stop the Criminalisation of Our Earth Defenders!\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date form of 'month/date/year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'\\b\\d{2}/\\d{2}/\\d{4}\\b')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%m/%d/%Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the container div for the article text\n",
    "    article_content = soup.find('div', class_='helpo_content_wrapper')\n",
    "    if not article_content:\n",
    "        print(\"Article content container not found.\")\n",
    "        return \"Could not find the article text.\"\n",
    "    \n",
    "    # Extract and return all text within the container\n",
    "    article_text = article_content.get_text(separator=\" \", strip=True)\n",
    "    return article_text\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename):\n",
    "    \"\"\"Crawl the articles between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "            \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_links = soup.findAll('div', {'class': 'elementor-post__text'})\n",
    "\n",
    "        for link in article_links:\n",
    "            time.sleep(1)\n",
    "            a_tag = link.find('a')\n",
    "            if not a_tag or 'href' not in a_tag.attrs:\n",
    "                continue\n",
    "            article_url = a_tag['href']\n",
    "            article_text = scrape_article(article_url)\n",
    "            # Extract the date from the span class if it exists\n",
    "            date_time = link.find('span', class_='elementor-post-date')\n",
    "            if date_time:\n",
    "                date_text = date_time.text\n",
    "                article_date = find_date_in_text(date_text)\n",
    "            else:\n",
    "                article_date = None\n",
    "\n",
    "            if article_date and start_date <= article_date <= end_date:\n",
    "                article_title = a_tag.text.strip()\n",
    "                writer.writerow([\"Racial Justice Network\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                print(f\"Saved article: {article_title}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://racialjusticenetwork.co.uk/blogs-and-articles/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Hastings Community of Sanctuary**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Joint Statement on Northeye: 4th September 2023\n",
      "Saved article: Why we oppose the plans for Northeye\n",
      "Saved article: East Sussex Library & Information Service gains Sanctuary Award\n"
     ]
    }
   ],
   "source": [
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_content = soup.find('section', class_='entry-content clearfix')\n",
    "        if article_content:\n",
    "            return article_content.get_text(separator=\" \", strip=True)\n",
    "    print(f\"Error or no content at {url}\")\n",
    "    return None\n",
    "\n",
    "def generate_date_urls(start_date, end_date):\n",
    "    \"\"\"Generate URLs for each day between start_date and end_date.\"\"\"\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        yield f\"https://hastings.cityofsanctuary.org/{current_date.strftime('%Y/%m/%d/')}\"\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "def find_articles_on_page(url):\n",
    "    \"\"\"Fetch all article links from a given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = []\n",
    "        article_links = soup.find_all('a', class_='post-link')\n",
    "        for article_link in article_links:\n",
    "            # The title is within an h4 tag, nested inside a div with class 'content'\n",
    "            title_tag = article_link.find('div', class_='content').find('h4', class_='title')\n",
    "            title = title_tag.get_text(strip=True) if title_tag else 'No Title Found'\n",
    "\n",
    "            # The link is the href attribute of the a tag\n",
    "            link = article_link.get('href')\n",
    "\n",
    "            if link:  # Only append if there's a valid link\n",
    "                articles.append((link, title))\n",
    "            else:\n",
    "                print(f\"Missing link for article titled '{title}' at {url}\")\n",
    "\n",
    "        return articles\n",
    "    else:\n",
    "        return []  # Return an empty list if the status code is not 200\n",
    "\n",
    "def extract_date_from_url(url):\n",
    "    \"\"\"Extract the date from the article URL based on format '/YYYY/MM/DD/'.\"\"\"\n",
    "    date_match = re.search(r'/(\\d{4}/\\d{2}/\\d{2})/', url)\n",
    "    if date_match:\n",
    "        return datetime.strptime(date_match.group(1), '%Y/%m/%d').strftime('%d %B %Y')\n",
    "    return \"Unknown Date\"  # Fallback if no date found\n",
    "\n",
    "def crawl_articles(start_date, end_date, csv_filename):\n",
    "    \"\"\"Crawl articles between the start and end dates.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "\n",
    "        writer = csv.writer(file)\n",
    "        for date_url in generate_date_urls(start_date, end_date):\n",
    "            time.sleep(1)  # Delay to respect the server\n",
    "            article_data = find_articles_on_page(date_url)\n",
    "            for article_url, article_title in article_data:\n",
    "                time.sleep(1)  # Respectful delay\n",
    "                article_text = scrape_article(article_url)\n",
    "                if article_text:\n",
    "                    article_date = extract_date_from_url(article_url)\n",
    "                    writer.writerow([\"Hastings Community of Sanctuary\", article_title, article_date, article_url, article_text])\n",
    "                    print(f\"Saved article: {article_title}\")\n",
    "\n",
    "\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Scottish Refugee Council**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 2...\n",
      "Saved article: Rwanda Bill: We urge MPs to reject legislation\n",
      "Saved article: Fair Begins Here: it’s time to make the asylum system work for everyone\n",
      "Saved article: Refugee Councils chosen for Guardian Charity Appeal\n",
      "Saved article: Housing emergency must end\n",
      "Saved article: Help refugees in need this winter\n",
      "Saved article: Rise: Refugee Festival Scotland 2024\n",
      "Saved article: Empowering Practitioners: Meet Our New Training Officer\n",
      "Scraping page 3...\n",
      "Saved article: Join us to celebrate Aref Ghorbani’s new EP No More Shadows\n",
      "Saved article: Rwanda ruling: Supreme Court rules against UK government\n",
      "Saved article: Hopes increase for free bus travel for people seeking asylum\n",
      "Saved article: Hotels are no place for children to live – calls to stop housing families and children in hotel rooms\n",
      "Saved article: Human rights and asylum in Scotland – government must act on findings in new report\n",
      "Saved article: A helping hand for families\n",
      "Saved article: Supporting people in crisis\n",
      "Saved article: Our statement on the humanitarian crisis in Gaza\n",
      "Saved article: Standing up for refugee rights\n",
      "Saved article: Reaching more people across Scotland\n",
      "Saved article: Helping refugees rebuild their lives\n",
      "Saved article: Helping New Scots Succeed\n",
      "Saved article: Nothing about refugees without refugees\n",
      "Saved article: Helping refugees find work in Scotland\n",
      "Saved article: Volunteering is a lifeline for people in the asylum system\n",
      "Saved article: Reimagine: New Scots art exhibition in Perthshire\n",
      "Scraping page 4...\n",
      "Saved article: Q&A: Concerns over huge number of people at risk of homelessness in Glasgow\n",
      "Saved article: New Scots Leaders: Mohammed’s story\n",
      "Saved article: Scottish Refugee Council Receives Investing in Volunteers Award\n",
      "Saved article: Afghans in Scotland: “Life here is not getting better for us”\n",
      "Saved article: Seven days to find a new home: unreasonable, unfair and completely unrealistic\n",
      "Saved article: Anna’s story\n",
      "Saved article: Fundraising ideas\n",
      "Saved article: Film screening: On our Doorstep\n",
      "Saved article: Cross Borders Mentoring: nurturing creative talent\n",
      "Saved article: Our mentoring project with Scottish Government\n",
      "Saved article: Love Glasgow Hate Racism fundraising concert\n",
      "Saved article: People seeking protection should not be forced to share rooms\n",
      "Saved article: Afroscots United: Champions on and off the pitch!\n",
      "Saved article: Rebuilding lives one step at a time\n",
      "Saved article: We must not forget Afghanistan\n",
      "Saved article: Afghanistan two years on: Stories from New Scots\n",
      "Scraping page 5...\n",
      "Saved article: Helping Afghan communities thrive\n",
      "Saved article: Flying kites of hope for Afghans in Inverness\n",
      "Saved article: Mykola’s story\n",
      "Saved article: Reaching New Scots Fund: evaluation report\n",
      "Saved article: The Ukrainian Collective\n",
      "Saved article: Learning from each other\n",
      "Saved article: Illegal Migration Act: Sign our open letter to the Prime Minister\n",
      "Saved article: Svitlana’s story\n",
      "Saved article: Solidarity with people seeking safety as the Illegal Migration bill becomes law\n",
      "Saved article: Illegal Migration bill set to become law\n",
      "Saved article: Ronnie’s story\n",
      "Saved article: The Illegal Migration Bill: What you need to know\n",
      "Saved article: Line-up for No Borders Comedy Night revealed SOLD OUT\n",
      "Saved article: Illegal Migration Bill sees series of defeats in House of Lords\n",
      "Saved article: ‘Inspirational’ Wafa Shaheen awarded honorary degree\n",
      "Saved article: Refugee Festival Scotland 2023 Comes to a Close\n",
      "Scraping page 6...\n",
      "Saved article: New Scots: building community and hope\n",
      "Saved article: World Refugee Day: Finding hope away from home\n",
      "Saved article: Everyday Hope\n",
      "Saved article: Our HOPE podcast\n",
      "Saved article: The Meaning of Hope: to be powerful is to be joyful\n",
      "Saved article: Refugee Festival Scotland is here!\n",
      "Saved article: Refugee Festival Scotland Media Awards winners announced\n",
      "Saved article: UK Government to remove two-tier asylum system\n",
      "Saved article: Guardianship Scotland to reach more at-risk children\n",
      "Saved article: Take action for hope this World Refugee Day\n",
      "Saved article: Shortlist for Refugee Festival Scotland Media Awards 2023 revealed\n",
      "Saved article: Research reveals link between government’s anti-migrant rhetoric and far-right activity\n",
      "Saved article: Refugee Festival Scotland 2023 Programme Now Live!\n",
      "Saved article: Sanctuary for people from Sudan\n",
      "Saved article: The House of Lords must reject ‘dehumanising and immoral’ Illegal Migration Bill\n",
      "Saved article: Media Awards open for entries\n",
      "Scraping page 7...\n",
      "Saved article: Be part of Refugee Festival Scotland\n",
      "Saved article: Supporting New Scots Fund announces funding\n",
      "Saved article: Army barracks and floating barges are not the solution to a problem of the government’s own making\n",
      "Saved article: Important information – changes to UK asylum rules\n",
      "Saved article: Funding boost for New Scot communities\n",
      "Saved article: MS Victoria to close on July 11th\n",
      "Saved article: Stories from Afghanistan in Perth\n",
      "Saved article: Scotland Rejects the Refugee Ban Bill\n",
      "Saved article: Stop the Refugee Ban Bill\n",
      "Saved article: Stand up to Racism on March 18th\n",
      "Saved article: Our response to the Home Office’s anti-refugee legislation\n",
      "Saved article: Open letter calls for amendments to ‘asylum questionnaire’\n",
      "Saved article: Supporting New Scots Fund opens for tenders\n",
      "Saved article: Our visit to the Bield’s creative arts residency\n",
      "Saved article: 12 groups supporting refugee women in Scotland\n",
      "Saved article: LGBT+ History Month 2023\n",
      "Scraping page 8...\n",
      "Saved article: Our CEO Sabir Zazai reflects on the anniversary of the war against Ukraine\n",
      "Saved article: From Ukraine to Scotland: Olga and Oleksii’s story\n",
      "Saved article: Home Secretary must stop far right targeting of asylum hotels\n",
      "Saved article: Sewing2gether for Ukraine\n",
      "Saved article: Ukraine response one year on\n",
      "Saved article: Demonstration planned to mark anniversary of war against Ukraine\n",
      "Saved article: Welcoming people fleeing Ukraine\n",
      "Saved article: Syria and Turkey earthquake\n",
      "Saved article: Giving Ukrainians in Scotland a voice\n",
      "Saved article: Supporting Ukrainian communities in Scotland\n",
      "Saved article: Call for party leaders to take stand against violence\n",
      "Saved article: Refugee Festival funding available for communities\n",
      "Saved article: Community Concern at Hotel Accommodation\n",
      "Saved article: Meet the Inclusive Homework Club\n",
      "Saved article: Calls for Rishi Sunak to stop placing children in hotels\n",
      "Saved article: Reaching New Scots Fund\n",
      "Scraping page 9...\n",
      "Saved article: Show some love for refugees this Valentine’s Day\n",
      "Saved article: Head of Scottish Refugee Council receives Queen’s honour\n",
      "Saved article: Photographing New Scots: a best practice guide\n",
      "Saved article: Urgent call for Rishi Sunak to fulfil promise to reunite Afghan families\n",
      "Saved article: Cruise ship housing over 1000 people from Ukraine to close\n",
      "Saved article: Forgotten Play will raise funds for refugees in Scotland\n",
      "Saved article: Rwanda: High Court rules that cruel scheme is lawful\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date in the format 'month date, year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'\\b([a-zA-Z]+) (\\d{1,2}), (\\d{4})\\b')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        date_str = f\"{match.group(1)} {match.group(2)}, {match.group(3)}\"  # Reformats the date to 'Month day, Year'\n",
    "        return datetime.strptime(date_str, '%B %d, %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text and publication date from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return None, \"\", \"\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract the main article content\n",
    "    article_content = soup.find('article', {'class': 'post-content'})\n",
    "    article_text = article_content.get_text(separator=' ', strip=True) if article_content else \"Could not find the article text.\"\n",
    "\n",
    "    # Extract the article title\n",
    "    title_container = soup.find('div', class_='my-3')\n",
    "    article_title = title_container.find('h1').get_text(strip=True) if title_container and title_container.find('h1') else \"Title Not Found\"\n",
    "\n",
    "    # Locate the date container, targeting the structure provided\n",
    "    date_container = soup.find('h6', class_='contforcat')\n",
    "    if date_container:\n",
    "        article_date_text = date_container.get_text(strip=True)\n",
    "        # Remove the prefix \"Posted on \" to isolate the date\n",
    "        article_date_text = article_date_text.replace('Posted on ', '')\n",
    "        article_date = find_date_in_text(article_date_text)\n",
    "    else:\n",
    "        article_date = None\n",
    "\n",
    "    return article_date, article_text, article_title\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=2, end_page=9):\n",
    "    \"\"\"Crawl the articles from page 2 to 9 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "        \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            time.sleep(1)\n",
    "            page_url = f\"{base_url}page/{page}\" \n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {page_url}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            article_links = soup.findAll('div', {'class': 'card'})\n",
    "\n",
    "            for link in article_links:\n",
    "                time.sleep(1)  # Polite delay between requests\n",
    "                a_tag = link.find('a')\n",
    "                if a_tag and a_tag['href']:\n",
    "                    article_url = a_tag['href']\n",
    "                    article_date, article_text, article_title = scrape_article(article_url)\n",
    "                                \n",
    "                    if article_date and start_date <= article_date <= end_date:\n",
    "                        writer.writerow([\"Scottish Refugee Council\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                        print(f\"Saved article: {article_title}\")\n",
    "\n",
    "\n",
    "# execution\n",
    "base_url = 'https://scottishrefugeecouncil.org.uk/news/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping *blogs* from **Jesuit Refugee Service UK**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 2...\n",
      "Saved article: At Home with the Jesuit Young Adult Ministry\n",
      "Saved article: Two years of impactful collaboration: JRS & World Medicine\n",
      "Saved article: “I’ve discovered friendship, resilience, good humour and triumph over adversity”\n",
      "Saved article: “Step back into time”\n",
      "Saved article: Volunteering with Acupuncture\n",
      "Saved article: £10 to stay connected\n",
      "Saved article: Why it’s important to observe World Day of Migrants and Refugees\n",
      "Saved article: Two months and already feeling At Home…\n",
      "Saved article: “To escape from our tumultuous lives and spend some time with Mother Nature”\n",
      "Scraping page 3...\n",
      "Saved article: Countering the hostile environment through hospitality\n",
      "Saved article: “It is better to light a candle than to curse the darkness”\n",
      "Saved article: My experience at JRS was nothing short of amazing, invaluable, and extremely eye opening.\n",
      "Saved article: From the Himalayas to Wapping with love\n",
      "Saved article: Creating community with the help of volunteers\n",
      "Saved article: “A day like that makes us think of something else”\n",
      "Saved article: A twentieth century inspiration – St Alberto Hurtado\n",
      "Saved article: St Ignatius of Loyola – a friend for all seasons; a universal friend\n",
      "Saved article: Solidarity and encounter in the face of the Illegal Migration Act\n",
      "Saved article: A reflection on the new JRS UK football team\n",
      "Scraping page 4...\n",
      "Saved article: My job helps me appreciate the things we often take for granted’\n",
      "Saved article: As it heads back to the House of Commons, what’s the state of the Illegal Migration – Refugee Ban – Bill?\n",
      "Saved article: A further update on the “Refugee Ban Bill”, July 2023\n",
      "Saved article: The availability of competent legal advice and representation for refugee friends is fast diminishing\n",
      "Saved article: How JRS’s Legal Project changed my life\n",
      "Saved article: What is Sanctuary Sunday? In the face of expanding detention and hostility, it is much needed\n",
      "Saved article: Creating messages of justice\n",
      "Saved article: Performing poetry this Refugee Week – putting joyful, funny, complex refugee voices at the centre of the narrative\n",
      "Saved article: Who’s that walking at my door?\n",
      "Saved article: Getting to know and love JRS, through volunteering\n",
      "Scraping page 5...\n",
      "Saved article: Coronation Community Kitchen: Cooking together for King and Country!\n",
      "Saved article: A brief update on the Illegal Migration Bill\n",
      "Saved article: Running the London Marathon: it exceeded all my expectations!\n",
      "Saved article: Social Justice in Action – Can you become a volunteer host?\n",
      "Saved article: Folkestone folk support the Napier report and the closure of Napier Barracks\n",
      "Saved article: How far can £15 go – in a fortnight?\n",
      "Saved article: Earth Day and JRS – a reflection\n",
      "Saved article: Cheering for Team JRS at the London Marathon 2023\n",
      "Saved article: Napier barracks shows we need communities, not camps\n",
      "Saved article: Reflecting on marathon training and the journey of those who were uprooted\n",
      "Scraping page 6...\n",
      "Saved article: What we know now, and what we’re still asking\n",
      "Saved article: In celebration of World Poetry Day, March 2023\n",
      "Saved article: Running the London Marathon for people who have had to flee\n",
      "Saved article: Embracing Equity in the darkest of places\n",
      "Saved article: What we know so far\n",
      "Saved article: World Book Day 2023\n",
      "Saved article: On the first anniversary of the invasion of Ukraine, Britain’s choices on refugee policy stand in sharp relief\n",
      "Saved article: Running the London Marathon 2023 to support refugees\n",
      "Saved article: Young Uruguayan visitors at JRS UK\n",
      "Saved article: JRS UK & Asylum Welcome talk about the impacts of immigration detention at the Oxford University Catholic Chaplaincy\n",
      "Scraping page 7...\n",
      "Saved article: Lord, were you there when I became a trafficked victim?\n",
      "Saved article: A catchup with the JRS UK Community Kitchen\n",
      "Saved article: It’s been a joy to wake up knowing: I’m going to JRS today!\n",
      "Saved article: In the face of growing hostility and adversity, where do we find our hope of the New Year? We find it in solidarity.\n",
      "Saved article: Irrespective of the High Court Judgment, sending asylum seekers to Rwanda for processing is deeply immoral\n",
      "Saved article: JRS Writing and Drama Group prepares for Advent Service performance\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date in the format 'date month year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'\\d{1,2} [a-zA-Z]+ \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%d %B %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text and publication date from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return None, \"\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract the main article content\n",
    "    article_content = soup.find('div', {'class': 'col100 singlearticletext'})\n",
    "    if article_content:\n",
    "        # Retrieve all text content from <p> tags within the article content\n",
    "        paragraphs = article_content.find_all('p')\n",
    "        article_text = ' '.join(p.get_text(strip=True) for p in paragraphs) if paragraphs else \"Could not find any text content.\"\n",
    "    else:\n",
    "        article_text = \"Could not find the article text.\"\n",
    "\n",
    "    # Extract the article title from the designated div class and h1\n",
    "    title_container = soup.find('div', class_='pageheadercolumn1row1content whitebg posrel')\n",
    "    article_title = title_container.find('h1').get_text(strip=True) if title_container and title_container.find('h1') else \"Title Not Found\"\n",
    "\n",
    "    return article_text, article_title\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=2, end_page=7):\n",
    "    \"\"\"Crawl the articles from page 2 to page 7 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "        \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            time.sleep(1)\n",
    "            page_url = f\"{base_url}page/{page}\" \n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {page_url}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            article_links = soup.findAll('div', {'class': 'newsitem lightbluebg'})\n",
    "\n",
    "            for link in article_links:\n",
    "                time.sleep(1)  # Polite delay between requests\n",
    "                # Find the 'newsitemtext' container within the 'newsitem lightbluebg' block\n",
    "                newsitem_text = link.find('div', class_='newsitemtext')\n",
    "                \n",
    "                if newsitem_text:\n",
    "                    # Find 'a' tag within 'h2' which is inside 'newsitemtext'\n",
    "                    a_tag = newsitem_text.find('h2').find('a') if newsitem_text.find('h2') else None\n",
    "                    if a_tag and 'href' in a_tag.attrs:\n",
    "                        article_url = a_tag['href']\n",
    "                        article_text, article_title = scrape_article(article_url)\n",
    "                        # Output or processing logic for the fetched article details\n",
    "\n",
    "                    # Find 'h4' within 'newsitemtext' for the date\n",
    "                    date_h4 = newsitem_text.find('h4')\n",
    "                    if date_h4:\n",
    "                        date_text = date_h4.get_text(strip=True)\n",
    "                        article_date = find_date_in_text(date_text)\n",
    "                    else:\n",
    "                        article_date = None\n",
    "                                \n",
    "                    if article_date and start_date <= article_date <= end_date:\n",
    "                        writer.writerow([\"Jesuit Refugee Service UK\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                        print(f\"Saved article: {article_title}\")\n",
    "\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.jrsuk.net/blog/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping *news* from **Jesuit Refugee Service UK**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Saved article: JRS UK calls for Bibby Stockholm to be closed following reported suicide\n",
      "Scraping page 2...\n",
      "Saved article: JRS UK welcomes Supreme Court Ruling on Rwanda\n",
      "Saved article: Write a Christmas Card for people held in immigration detention\n",
      "Saved article: Advent Service 2023\n",
      "Saved article: JRS UK Opens Amani House for Male Refugee Friends\n",
      "Saved article: JRS responds to the reports of the contents of the Home Secretary’s speech to the American Enterprise Institute later today\n",
      "Saved article: The Jesuit Refugee Service (JRS) UK renews calls for an end immigration detention in wake of report on abuse in Brook House IRC\n",
      "Saved article: People have now been moved onto the Bibby Stockholm\n",
      "Saved article: Illegal Migration Bill becomes UK Law\n",
      "Saved article: Rwanda Scheme found to be UNLAWFUL!\n",
      "Saved article: “My thoughts are my secret power”: You can see me, but I don’t exist\n",
      "Scraping page 3...\n",
      "Saved article: JRS UK renews calls for government to abandon Illegal Migration Bill as JCHR report finds the Bill would “deny the vast majority of refugees access to the UK’s asylum system”\n",
      "Saved article: Join JRS for Refugee Week 2023\n",
      "Saved article: Welcoming the Stranger – Information Session for JRS UK’s At Home Hosting Scheme\n",
      "Saved article: Folkestone launch of the Napier Report\n",
      "Saved article: New Report – Napier Barracks: the inhumane reality\n",
      "Saved article: Pray Stations of the Cross online with JRS UK this Lent 2023\n",
      "Saved article: Love the Stranger\n",
      "Saved article: Illegal Migration Bill\n",
      "Saved article: Challenge this violence and hostility by community building\n",
      "Saved article: JRS UK renews calls for end to detention in wake of ICIBI report* calling out ineffectiveness of safeguards for vulnerable people detained people\n",
      "Scraping page 4...\n",
      "Saved article: A wonderful occasion to be together and celebrate the talents of our refugee friends\n",
      "Saved article: JRS UK renews calls for end to plan to force refugees to Rwanda in wake of High Court’s Judgement\n",
      "Saved article: JRS UK excoriate Government’s latest announcement doubling down on plans to make asylum impossible\n",
      "Saved article: 14 December 2022 marks six months since the first flight to Rwanda was prevented from departing\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date in the format 'date month year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'\\d{1,2} [a-zA-Z]+ \\d{4}')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%d %B %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text and publication date from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return None, \"\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract the main article content\n",
    "    article_content = soup.find('div', {'class': 'col100 singlearticletext'})\n",
    "    if article_content:\n",
    "        # Retrieve all text content from <p> tags within the article content\n",
    "        paragraphs = article_content.find_all('p')\n",
    "        article_text = ' '.join(p.get_text(strip=True) for p in paragraphs) if paragraphs else \"Could not find any text content.\"\n",
    "    else:\n",
    "        article_text = \"Could not find the article text.\"\n",
    "\n",
    "    # Extract the article title from the designated div class and h1\n",
    "    title_container = soup.find('div', class_='pageheadercolumn1row1content whitebg posrel')\n",
    "    article_title = title_container.find('h1').get_text(strip=True) if title_container and title_container.find('h1') else \"Title Not Found\"\n",
    "\n",
    "    return article_text, article_title\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=1, end_page=4):\n",
    "    \"\"\"Crawl the articles from page 1 to page 4 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "        \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            time.sleep(1)\n",
    "            page_url = f\"{base_url}page/{page}\" \n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {page_url}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            article_links = soup.findAll('div', {'class': 'newsitem blue3bg whitetext'})\n",
    "\n",
    "            for link in article_links:\n",
    "                time.sleep(1)  # Polite delay between requests\n",
    "                # Find the 'newsitemtext' container within the 'newsitem blue3bg whitetext' block\n",
    "                newsitem_text = link.find('div', class_='newsitemtext')\n",
    "                \n",
    "                if newsitem_text:\n",
    "                    # Find 'a' tag within 'h2' which is inside 'newsitemtext'\n",
    "                    a_tag = newsitem_text.find('h2').find('a') if newsitem_text.find('h2') else None\n",
    "                    if a_tag and 'href' in a_tag.attrs:\n",
    "                        article_url = a_tag['href']\n",
    "                        article_text, article_title = scrape_article(article_url)\n",
    "                        # Output or processing logic for the fetched article details\n",
    "\n",
    "                    # Find 'h4' within 'newsitemtext' for the date\n",
    "                    date_h4 = newsitem_text.find('h4')\n",
    "                    if date_h4:\n",
    "                        date_text = date_h4.get_text(strip=True)\n",
    "                        article_date = find_date_in_text(date_text)\n",
    "                    else:\n",
    "                        article_date = None\n",
    "                                \n",
    "                    if article_date and start_date <= article_date <= end_date:\n",
    "                        writer.writerow([\"Jesuit Refugee Service UK\", article_title, article_date.strftime('%d %B %Y'), article_url, article_text])\n",
    "                        print(f\"Saved article: {article_title}\")\n",
    "\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.jrsuk.net/news/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **ECPAT UK**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Home Affairs Committee speaks out against Home Office treatment of migrant children and calls for increased care for child victims of trafficking in new report.\n",
      "Saved article: Judge rules that the Home Secretary acted irrationally in the case brought by Kent County Council on the operation of the National Transfer Scheme\n",
      "Saved article: King’s Speech announces new laws to tackle online child exploitation whilst reconfirming the UK government’s desire to remove the rights of child victims\n",
      "Saved article: ECPAT UK welcomes the announcement of a new Independent Anti-Slavery Commissioner\n",
      "Saved article: High Court orders Home Office to transfer unaccompanied children from hotels into care\n",
      "Saved article: High Court retains oversight of the treatment of unaccompanied children\n",
      "Saved article: ECPAT UK wins legal challenge on the unlawful accommodation of unaccompanied children\n",
      "Saved article: ECPAT UK joined court hearing for its legal challenge to Kent County Council and the Home Secretary on the accommodation of unaccompanied children to be heard on 20th and 21st July 2023\n",
      "Saved article: ECPAT UK Youth Advisory Group writes to parliamentarians about the risks to children in the Illegal Migration Bill\n",
      "Saved article: ECPAT UK launches legal challenge to Kent County Council and the Home Secretary on the accommodation of unaccompanied children\n",
      "Saved article: Refugee Month - join us in calling for #CareForEveryChild\n",
      "Saved article: ECPAT UK secures support from Propel funding collaboration to reach more young victims\n",
      "Saved article: Care for every child: Duties to care for children must apply equally to all children\n",
      "Saved article: Over 100 charities call for action on children going missing from Home Office hotels, at risk of trafficking and exploitation\n",
      "Saved article: As UN Experts express alarm, coalition calls on the Government to end hostility towards survivors of trafficking and modern slavery\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_html_element(soup):\n",
    "    \"\"\"Extracts the date from a 'time' HTML element's 'datetime' attribute within the soup.\"\"\"\n",
    "    date_element = soup.find('time', id='time')\n",
    "    if date_element and 'datetime' in date_element.attrs:\n",
    "        try:\n",
    "            date = datetime.strptime(date_element['datetime'], '%Y-%m-%d').strftime('%d %B %Y')\n",
    "            return date\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    # Ensure URL starts with http:// or https://\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = base_url.rstrip('/') + '/' + url.lstrip('/')\n",
    "    \n",
    "    # Correcting potential duplicate path issues\n",
    "    url = url.replace('/news/news/', '/news/')\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}, Status Code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    article_content = soup.find('div', class_='content postContent newsContent')\n",
    "    if not article_content:\n",
    "        print(\"Article content container not found.\")\n",
    "        return None\n",
    "\n",
    "    return article_content.get_text(separator=\" \", strip=True)\n",
    "\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename):\n",
    "    \"\"\"Crawl the articles between the start and end dates.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "            \n",
    "        writer = csv.writer(file)\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles = soup.findAll('div', {'class': 'listedPostText'})\n",
    "\n",
    "        for block in articles:\n",
    "            a_tag = block.find('a')\n",
    "            if not a_tag or 'href' not in a_tag.attrs:\n",
    "                continue\n",
    "\n",
    "            article_url = a_tag['href']\n",
    "            article_text = scrape_article(article_url)\n",
    "            article_title = a_tag.get_text(strip=True)\n",
    "\n",
    "            # Attempt to find the date element by navigating to the common parent\n",
    "            details_wrapper = block.parent.find('div', class_='publishDetailsWrapper')\n",
    "            if details_wrapper:\n",
    "                article_date = find_date_in_html_element(details_wrapper)\n",
    "            else:\n",
    "                article_date = None\n",
    "\n",
    "            if article_date:\n",
    "                article_date_obj = datetime.strptime(article_date, '%d %B %Y')\n",
    "                if start_date <= article_date_obj <= end_date:\n",
    "                    full_url = 'https://www.ecpat.org.uk' + article_url\n",
    "                    writer.writerow([\"ECPAT UK\", article_title, article_date, full_url, article_text])\n",
    "                    print(f\"Saved article: {article_title}\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.ecpat.org.uk/news'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Migrants Organise**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 2 at https://www.migrantsorganise.org/news/page/2\n",
      "Scraping article at https://www.migrantsorganise.org/homes4all/\n",
      "Scraping article at https://www.migrantsorganise.org/hope-a-migrants-organise-exhibition/\n",
      "Scraping article at https://www.migrantsorganise.org/group-activities-nourishment-and-care/\n",
      "Scraping article at https://www.migrantsorganise.org/our-2023-round-up/\n",
      "Scraping article at https://www.migrantsorganise.org/wewontstop/\n",
      "Scraping article at https://www.migrantsorganise.org/new-guide-challenging-notice-to-move-to-the-bibby-stockholm-barge/\n",
      "Saved article: New Guide: Challenging Notice to Move to the Bibby Stockholm Barge\n",
      "Scraping article at https://www.migrantsorganise.org/celebrating-30-years-of-migrants-organise/\n",
      "Saved article: Celebrating 30 Years of Migrants Organise\n",
      "Scraping article at https://www.migrantsorganise.org/endcolonialcomplicity-palestine-solidarity-statement/\n",
      "Saved article: #EndColonialComplicity: Palestine Solidarity Statement\n",
      "Scraping article at https://www.migrantsorganise.org/the-justice-for-omisha-campaign-get-involved/\n",
      "Saved article: The Justice for Omisha Campaign – Get Involved!\n",
      "Scraping page 3 at https://www.migrantsorganise.org/news/page/3\n",
      "Scraping article at https://www.migrantsorganise.org/solidarity-knows-no-borders-national-summit-2023/\n",
      "Saved article: Solidarity Knows No Borders National Summit 2023\n",
      "Scraping article at https://www.migrantsorganise.org/bibbybargeupdate/\n",
      "Saved article: Legal Briefing—Legal Challenge Against the Bibby Stockholm Barge\n",
      "Scraping article at https://www.migrantsorganise.org/connecting-climate-justice-migrant-justice-a-guide-to-countering-dangerous-narratives-report-launch/\n",
      "Saved article: Connecting Climate Justice & Migrant Justice: A Guide to Countering Dangerous Narratives – Report Launch!\n",
      "Scraping article at https://www.migrantsorganise.org/were-hiring-access-to-justice-organiser/\n",
      "Saved article: We’re Hiring! Access to Justice Organiser\n",
      "Scraping article at https://www.migrantsorganise.org/press-release-migrants-organise-takes-action-against-home-office-over-unsafe-bibby-stockholm-prison-barge/\n",
      "Saved article: Press Release: Migrants Organise takes action against Home Office over unsafe Bibby Stockholm prison barge\n",
      "Scraping article at https://www.migrantsorganise.org/bibby-stockholm-will-never-be-safe-this-is-why-were-taking-action/\n",
      "Saved article: Bibby Stockholm will never be safe. This is why we’re taking action.\n",
      "Scraping article at https://www.migrantsorganise.org/unions-migrant-organisations-statement-public-sector-pay-rise/\n",
      "Saved article: Unions & Migrant Organisations’ statement – public sector pay rise\n",
      "Scraping article at https://www.migrantsorganise.org/people-not-boats/\n",
      "Saved article: They are people, not boats: Speaking out against the Refugee Ban\n",
      "Scraping article at https://www.migrantsorganise.org/report-launch-housing-in-a-hostile-environment/\n",
      "Saved article: Report Launch: Housing In A Hostile Environment\n",
      "Scraping page 4 at https://www.migrantsorganise.org/news/page/4\n",
      "Scraping article at https://www.migrantsorganise.org/memberspeaksout/\n",
      "Saved article: Resilience Festival: “This is how we build hope”\n",
      "Scraping article at https://www.migrantsorganise.org/migrants-organise-members-are-telling-clearsprings-we-demand-decent-homes/\n",
      "Saved article: Migrants Organise Members Are Telling Clearsprings: We Demand Decent Homes!\n",
      "Scraping article at https://www.migrantsorganise.org/migrants-organise-resilience-festival-2023-gets-started/\n",
      "Saved article: Migrants Organise Resilience Festival 2023 gets started!\n",
      "Scraping article at https://www.migrantsorganise.org/talk-tv-issues-apology-and-pays-substantial-damages-to-migrants-organise-for-defamatory-statements/\n",
      "Saved article: Talk TV Issues Apology and Pays Substantial Damages to Migrants Organise for Defamatory Statements\n",
      "Scraping article at https://www.migrantsorganise.org/were-walking-for-justice-can-you-donate/\n",
      "Saved article: We’re walking 10 km for justice- can you donate?\n",
      "Scraping article at https://www.migrantsorganise.org/updated-guidance-and-flyers-to-challenge-reporting-conditions/\n",
      "Saved article: Updated Resources to Challenge Reporting Conditions\n",
      "Scraping article at https://www.migrantsorganise.org/article-supporting-people-with-immigration-issues-in-the-context-of-the-mental-health-act-1983-and-mental-capacity-act-2005/\n",
      "Saved article: ARTICLE: Supporting people with immigration issues in the context of the Mental Health Act 1983 and Mental Capacity Act 2005\n",
      "Scraping article at https://www.migrantsorganise.org/charities-protest-capita-agm-over-controversial-government-gps-tracking-contract/\n",
      "Saved article: Charities protest Capita AGM over controversial government GPS tracking contract\n",
      "Scraping article at https://www.migrantsorganise.org/yorkshire-solidarity-summit/\n",
      "Saved article: Yorkshire Solidarity Summit\n",
      "Scraping page 5 at https://www.migrantsorganise.org/news/page/5\n",
      "Scraping article at https://www.migrantsorganise.org/the-refugee-ban-bill-harms-us-all/\n",
      "Saved article: The ‘Refugee Ban Bill’ harms us all\n",
      "Scraping article at https://www.migrantsorganise.org/no-borders-in-the-nhs-at-the-sos-nhs-national-demonstration/\n",
      "Saved article: No Borders in the NHS at the SOS NHS National Demonstration!\n",
      "Scraping article at https://www.migrantsorganise.org/stand-up-speak-out-solidarity-knows-no-borders-training-series-a-toolkit-to-resist-the-hostile-environment%ef%bf%bc/\n",
      "Saved article: Stand Up! Speak Out! Solidarity Knows No Borders Training Series: A Toolkit to Resist the Hostile Environment￼\n",
      "Scraping article at https://www.migrantsorganise.org/we-have-rights-and-we-need-to-know-them/\n",
      "Saved article: “We have rights. And we need to know them.” Lola speaks out\n",
      "Scraping article at https://www.migrantsorganise.org/a-victory-for-our-housing-group/\n",
      "Saved article: A victory for our Housing Group…\n",
      "Scraping article at https://www.migrantsorganise.org/migrants-organise-members-speak-out/\n",
      "Saved article: Migrants Organise Members Speak Out…\n",
      "Scraping article at https://www.migrantsorganise.org/hackney-patients-not-passports-screening-nhs-borderlands-at-homerton-hospital-16th-feb-2023/\n",
      "Saved article: Hackney Patients Not Passports Screening: NHS Borderlands at Homerton Hospital – 16th Feb 2023\n",
      "Scraping article at https://www.migrantsorganise.org/press-release-local-campaign-wins-urgent-repairs-for-unlivable-asylum-accommodation-in-tower-hamlets/\n",
      "Scraping article at https://www.migrantsorganise.org/donate-to-our-winter-solidarity-appeal/\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date in the format 'Month dayth, year' from the text.\"\"\"\n",
    "    # Updated regex to handle different day suffixes and potential extra spaces\n",
    "    pattern = re.compile(r'\\b([a-zA-Z]+)\\s+(\\d{1,2})(st|nd|rd|th),\\s+(\\d{4})\\b')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        month, day, _, year = match.groups()\n",
    "        # Construct date string without the ordinal suffix and handle possible leading zeros in day\n",
    "        date_str = f\"{month} {int(day)} {year}\"\n",
    "        # Parse the date in the desired output format\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(date_str, '%B %d %Y').strftime('%d %B %Y')\n",
    "            return parsed_date\n",
    "        except ValueError as e:\n",
    "            print(f\"Date parsing error: {e} with date_str: {date_str}\")\n",
    "    else:\n",
    "        print(f\"No date match found in text: {text}\")\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text and publication date from the given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return None, \"\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract the main article content\n",
    "    article_content = soup.find('div', {'class': 'entry-content'})\n",
    "    article_text = article_content.get_text(separator=' ', strip=True) if article_content else \"Could not find the article text.\"\n",
    "\n",
    "    # Extract the article title from the designated header class\n",
    "    title_container = soup.find('h1', class_='entry-title')\n",
    "    article_title = title_container.get_text(strip=True) if title_container else \"Title Not Found\"\n",
    "\n",
    "    return article_text, article_title\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename, start_page=2, end_page=5):\n",
    "    \"\"\"Crawl the articles from page 2 to page 5 that were published between the start and end dates.\"\"\"\n",
    "    \"\"\"Save the organisation name, article title, date, link and text to a CSV file.\"\"\"\n",
    "    \"\"\"Delay 1 second between requests to be polite.\"\"\"\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:  # 'a' mode for appending\n",
    "        file.seek(0, os.SEEK_END)\n",
    "        if file.tell() != 0:  # File is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "        \n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            page_url = f\"{base_url}page/{page}\"\n",
    "            print(f\"Scraping page {page} at {page_url}\")\n",
    "            response = requests.get(page_url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {page_url}\")\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            article_links = soup.findAll('div', class_='card-body p-0 latest-news')\n",
    "\n",
    "            for link in article_links:\n",
    "                time.sleep(1)\n",
    "                h4 = link.find('h4')\n",
    "                if not h4:\n",
    "                    print(\"No h4 element found within article link div.\")\n",
    "                    continue\n",
    "                a_tag = h4.find('a')\n",
    "                if not a_tag or 'href' not in a_tag.attrs:\n",
    "                    print(\"No valid a tag found or missing href.\")\n",
    "                    continue\n",
    "                article_url = a_tag['href']\n",
    "                print(f\"Scraping article at {article_url}\")\n",
    "\n",
    "                article_text, article_title = scrape_article(article_url)\n",
    "                date_container = link.find_next_sibling('div', class_='card-footer px-0')\n",
    "                if not date_container:\n",
    "                    print(\"No date container found for this article.\")\n",
    "                    continue\n",
    "\n",
    "                date_text = date_container.get_text(strip=True)\n",
    "                article_date = find_date_in_text(date_text)\n",
    "                if not article_date:\n",
    "                    print(\"No valid date extracted.\")\n",
    "                    continue\n",
    "\n",
    "                if article_date and start_date <= datetime.strptime(article_date, '%d %B %Y') <= end_date:\n",
    "                    writer.writerow([\"Migrants Organise\", article_title, article_date, article_url, article_text])\n",
    "                    print(f\"Saved article: {article_title}\")\n",
    "\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.migrantsorganise.org/news/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Safe Passage**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: A #MessageInABottle for refugees\n",
      "Saved article: Our reaction to supreme court ruling on Rwanda plan\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date in the format 'day Month year'.\"\"\"\n",
    "    pattern = re.compile(r'\\b(\\d{1,2})\\s([A-Z][a-z]+)\\s(\\d{4})\\b')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        # Format the date as 'day Month Year' and return\n",
    "        return datetime.strptime(match.group(0), '%d %B %Y').strftime('%d %B %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the article text from the given URL.\"\"\"\n",
    "    if not url.startswith(('http://', 'https://')):\n",
    "        url = base_url.rstrip('/') + '/' + url.lstrip('/')\n",
    "    # Correcting potential duplicate path issues\n",
    "        url = url.replace('/news/news/', '/news/')\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the container div for the article text\n",
    "    article_content = soup.find('div', class_='sqs-layout sqs-grid-12 columns-12')\n",
    "    if not article_content:\n",
    "        print(\"Article content container not found.\")\n",
    "        return \"Could not find the article text.\"\n",
    "    \n",
    "    # Extract and return all text within the container\n",
    "    article_text = article_content.get_text(separator=\" \", strip=True)\n",
    "    return article_text\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename):\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:\n",
    "        if file.tell() != 0:  # Check if the file is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_links = soup.findAll('a', {'class': 'BlogList-item-title'})\n",
    "\n",
    "        for a_tag in article_links:\n",
    "            article_title = a_tag.get_text(strip=True)\n",
    "            article_url = a_tag['href']\n",
    "\n",
    "            article_title = a_tag.text.strip()\n",
    "            full_url = 'https://www.safepassage.org.uk' + article_url\n",
    "\n",
    "            date_tag = a_tag.find_next('time', class_='Blog-meta-item--date')\n",
    "            if date_tag:\n",
    "                date_text = date_tag.get_text(strip=True)\n",
    "                article_date = find_date_in_text(date_text)\n",
    "                if article_date and start_date <= datetime.strptime(article_date, '%d %B %Y') <= end_date:\n",
    "                    article_text = scrape_article(full_url)\n",
    "                    writer.writerow([\"Safe Passage\", article_title, article_date, full_url, article_text])\n",
    "                    print(f\"Saved article: {article_title}\")\n",
    "            else:\n",
    "                print(\"No date tag found for this article.\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.safepassage.org.uk/news/'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape articles from **Safe Passage**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Statement on the Palestine/Israel conflict\n",
      "Saved article: Empty promises, shattered lives: the truth about Afghan resettlement schemes\n",
      "Saved article: Routes to safety: a new approach to people crossing the Channel\n",
      "Saved article: Safe Passage responds to Suella Braverman's statement on refugee protection\n",
      "Saved article: Responding to Labour's promise to restore the right to asylum\n",
      "Saved article: More loss of life at sea in search of safety\n",
      "Saved article: Responding to tragic loss of life in the Channel\n",
      "Saved article: Our response to the Illegal Migration Bill becoming law\n",
      "Saved article: Our response to tragic shipwreck off Greece\n",
      "Saved article: Announcement on leadership at Safe Passage\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date in the format 'day Month year'.\"\"\"\n",
    "    pattern = re.compile(r'\\b(\\d{1,2})\\s([A-Z][a-z]+)\\s(\\d{4})\\b')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        # Format the date as 'day Month Year' and return\n",
    "        return datetime.strptime(match.group(0), '%d %B %Y').strftime('%d %B %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the artitextcle  from the given URL.\"\"\"\n",
    "    base_url_cleaned = base_url.split('?')[0]  # Remove query parameters\n",
    "    url = base_url_cleaned + url\n",
    "    # Correcting potential duplicate path issues\n",
    "    url = url.replace('/news/news/', '/news/')\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the container div for the article text\n",
    "    article_content = soup.find('div', class_='sqs-layout sqs-grid-12 columns-12')\n",
    "    if not article_content:\n",
    "        print(\"Article content container not found.\")\n",
    "        return \"Could not find the article text.\"\n",
    "    \n",
    "    # Extract and return all text within the container\n",
    "    article_text = article_content.get_text(separator=\" \", strip=True)\n",
    "    return article_text\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename):\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:\n",
    "        if file.tell() != 0:  # Check if the file is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_links = soup.findAll('a', {'class': 'BlogList-item-title'})\n",
    "\n",
    "        for a_tag in article_links:\n",
    "            article_title = a_tag.get_text(strip=True)\n",
    "            article_url = a_tag['href']\n",
    "\n",
    "            article_title = a_tag.text.strip()\n",
    "            full_url = 'https://www.safepassage.org.uk' + article_url\n",
    "\n",
    "            date_tag = a_tag.find_next('time', class_='Blog-meta-item--date')\n",
    "            if date_tag:\n",
    "                date_text = date_tag.get_text(strip=True)\n",
    "                article_date = find_date_in_text(date_text)\n",
    "                if article_date and start_date <= datetime.strptime(article_date, '%d %B %Y') <= end_date:\n",
    "                    article_text = scrape_article(article_url)\n",
    "                    writer.writerow([\"Safe Passage\", article_title, article_date, full_url, article_text])\n",
    "                    print(f\"Saved article: {article_title}\")\n",
    "            else:\n",
    "                print(\"No date tag found for this article.\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://www.safepassage.org.uk/news?offset=1700045454524'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: Government's new Bill to stop Channel crossings will not work\n",
      "Saved article: Campaigning to #ReuniteAfghanFamilies\n",
      "Saved article: Dear Prime Minister, help Afghan families reunite\n",
      "Saved article: Campaigner Spotlight: Farhad's Dissertation\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://www.safepassage.org.uk/news?offset=1685633008001'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping articles from **Kent Refugee Action Network**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: making it all worthwhile\n",
      "Saved article: Leading the way to achievement\n",
      "Saved article: more than just a mentor\n",
      "Saved article: statement on RWANDA ruling\n",
      "Saved article: “kindness is WHAT kran DOES”\n",
      "Saved article: WHY collaboration is key\n",
      "Saved article: struggle hits home\n",
      "Saved article: from couch to 26.2 miles!\n",
      "Saved article: DOING IT FOR OURSELVES\n",
      "Saved article: loach: hope not hate\n",
      "Saved article: gone but never forgotten\n",
      "Saved article: Giving peace a chance\n",
      "Saved article: degree dream come true\n",
      "Saved article: wheelie big success!\n",
      "Saved article: DoAA means business\n",
      "Saved article: sharing Dr King’s dream\n",
      "Saved article: No end to their talents!\n"
     ]
    }
   ],
   "source": [
    "def find_date_in_text(text):\n",
    "    \"\"\"Define and apply a regex pattern to extract the date from of 'date/month/year' from the text.\"\"\"\n",
    "    pattern = re.compile(r'\\b\\d{2}/\\d{2}/\\d{4}\\b')\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%d/%m/%Y').strftime('%d %B %Y')\n",
    "    return None\n",
    "\n",
    "def scrape_article(url):\n",
    "    \"\"\"Scrape the artitextcle  from the given URL.\"\"\"\n",
    "    base_url_cleaned = base_url.split('?')[0]  # Remove query parameters\n",
    "    url = base_url_cleaned + url\n",
    "    # Correcting potential duplicate path issues\n",
    "    url = url.replace('/lateststories/lateststories/', '/lateststories/')\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching article: {url}\")\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the container div for the article text\n",
    "    article_content = soup.find('div', class_='sqs-html-content')\n",
    "    if not article_content:\n",
    "        print(\"Article content container not found.\")\n",
    "        return \"Could not find the article text.\"\n",
    "    \n",
    "    # Extract and return all text within the container. This includes text within <span> tags.\n",
    "    article_text = article_content.get_text(separator=\" \", strip=True)\n",
    "    return article_text\n",
    "\n",
    "def crawl_articles(start_date, end_date, base_url, csv_filename):\n",
    "    start_date = datetime.strptime(start_date, '%d %B %Y')\n",
    "    end_date = datetime.strptime(end_date, '%d %B %Y')\n",
    "\n",
    "    with open(csv_filename, 'a', newline='', encoding='utf-8') as file:\n",
    "        if file.tell() != 0:  # Check if the file is not empty\n",
    "            file.write('\\n')  # Ensure starts on a new line\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        response = requests.get(base_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {base_url}\")\n",
    "            return\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        article_links = soup.findAll('div', {'class': 'blog-item-text'})\n",
    "\n",
    "        for link in article_links:\n",
    "            time.sleep(2)\n",
    "            h1_tag = link.find('h1', class_='blog-title')  # Assuming this is where the link is\n",
    "            if h1_tag:\n",
    "                a_tag = h1_tag.find('a')\n",
    "                if a_tag and 'href' in a_tag.attrs:\n",
    "                    article_url = a_tag['href']\n",
    "                    full_url = 'https://kran.org.uk' + article_url\n",
    "                    article_title = a_tag.get_text(strip=True)\n",
    "\n",
    "                    # Finding the date in the neighboring meta section\n",
    "                    meta_section = link.parent.find('div', class_='blog-meta-section')\n",
    "                    if meta_section:\n",
    "                        date_tag = meta_section.find('time', class_='blog-date')\n",
    "                        if date_tag:\n",
    "                            date_text = date_tag.get_text(strip=True)\n",
    "                            article_date = find_date_in_text(date_text)\n",
    "                            if article_date and start_date <= datetime.strptime(article_date, '%d %B %Y') <= end_date:\n",
    "                                article_text = scrape_article(article_url)\n",
    "                                writer.writerow([\"Kent Refugee Action Network\", article_title, article_date, full_url, article_text])\n",
    "                                print(f\"Saved article: {article_title}\")\n",
    "                        else:\n",
    "                            print(\"No date tag found for this article.\")\n",
    "                    else:\n",
    "                        print(\"No meta section found for this article.\")\n",
    "\n",
    "# execution\n",
    "base_url = 'https://kran.org.uk/lateststories?offset=1702996817564'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved article: looking beyond labels\n",
      "Saved article: be part of real change\n",
      "Saved article: left with no choice\n",
      "Saved article: there is always hope\n",
      "Saved article: bill’s DEVASTATING IMPACT\n",
      "Saved article: first-class time at kran\n",
      "Saved article: this journey is difficult\n",
      "Saved article: new life and hope\n",
      "Saved article: helping to reignite hope\n",
      "Saved article: noteworthy partnership\n",
      "Saved article: One team, one dream\n",
      "Saved article: Mental health matters\n",
      "Saved article: Happy Holidays!\n",
      "Saved article: KRAN Bike Project\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://kran.org.uk/lateststories?offset=1692700106397'\n",
    "csv_filename = '/Users/yijingxiao/Desktop/ASDS dissertation/dissertation-data/article_text.csv'\n",
    "crawl_articles('13 December 2022', '12 December 2023', base_url, csv_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
