---
title: "text analysis"
output: html_document
date: "2024-05-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(quanteda.sentiment)
library(stringr)
library(stm)
library(lubridate)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(RColorBrewer)
library(patchwork)
library(data.table)
library(word2vec)
library(textTinyR)
library(reshape2)
library(scales)
library(grid)
library(gridExtra)
library(magick)
```

## Data Loading and Preprocessing
```{r}
articles <- read.csv("article_text.csv")

# Creating a day of the year column for time series analysis
articles$Date <- dmy(articles$Date)
articles$day_of_year <- yday(articles$Date)

summary(articles)
head(articles)
```
## Descriptive Analysis
```{r}
# Original article text
texts <- articles$Text

# Remove lengthy disclaimers from several organisations directly from the dataset
disclaimer_pattern <- disclaimer_pattern <- "Disclaimer.*?All rights reserved|Share this:.*?Loading... Related|\\[All News\\]|Share this page.*?Twitter"

# Use str_remove_all to remove patterns from each text
cleaned_texts <- str_remove_all(texts, disclaimer_pattern)

# Remove the rows with empty texts
non_empty <- cleaned_texts != ""
cleaned_texts <- cleaned_texts[non_empty]
articles <- articles[non_empty, ]

# Create corpus from cleaned texts, set 4 docvariables associated with each document
corp <- corpus(cleaned_texts,
               docvars = data.frame(organisation = articles$Organisation,
                                    title = articles$Title,
                                    date = articles$Date,
                                    day_of_year = articles$day_of_year))

# Create the dfm, remove stopwords, punctuation, numbers, symbols, and URLs, and stem the words
dfm <- corp %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,
         remove_url = TRUE) %>%
  tokens_remove(stopwords("en")) %>%
  tokens_wordstem() %>%
  tokens_ngrams(n = 1:2) %>% # include unigrams and bigrams
  dfm() %>%
  dfm_trim(min_termfreq = 5) # remove terms that appear in fewer than 5 documents

topfeatures(dfm, n = 50)

ndoc(dfm) # total number of documents
sum(dfm) # total number of tokens
nfeat(dfm) # total number of types/features

# Save the plot
png("Wordcloud.png", width = 16, height = 10, units = "in", res = 300)

# plot the wordcloud
textplot_wordcloud(dfm, min_count = 10, max_words = 150, rotation = 0)

dev.off()

# calculate tf-idf scores for each feature
dfm_tfidf <- dfm_tfidf(dfm)
# print the top 10 words with the highest tf-idf scores
top_features <- topfeatures(dfm_tfidf, 10)

top_features_table <- data.frame(term = names(top_features), tfidf = top_features, row.names = NULL)

# Print the table
print(top_features_table)

```
## Structural Topic Modelling
Based on the diagnostic plots, selecting K = 15 appears optimal, as it captures a balance between maximising Held-Out Likelihood and maintaining acceptable Semantic Coherence. At this point, the likelihood indicates an efficient model fit without overfitting, evidenced by a notable peak in likelihood and acceptable coherence levels. Additionally, K = 15 shows diminishing returns in model improvement beyond this point, as evidenced by the steep decline in Semantic Coherence and minimal gains in the Lower Bound for higher values of K. This choice ensures a robust model that interprets distinct topics with substantial coherence and relevance.
```{r}
# Convert the quanteda object into a stm input
stm_input <- convert(dfm, to = "stm")
# Quantitative diagnostics
k_search_output <- searchK(stm_input$documents, stm_input$vocab,
                           K = c(5, 10, 15, 20, 25), data = stm_input$meta,
                           verbose = FALSE, heldout.seed = 123)
plot(k_search_output)
k_search_output
```

```{r}
stmodel <- stm(documents = stm_input$documents, vocab = stm_input$vocab,
                     K = 15, prevalence = ~ s(day_of_year),
               data = stm_input$meta, verbose = FALSE, init.type = "LDA", seed = 123) # initialise the structural topic model with LDA as it works better in this scenario
plot(stmodel)
```

```{r}
cloud(stmodel, topic = 1, scale = c(2, .25)) # Migrants Diversity and Historical Acknowledgement
cloud(stmodel, topic = 2, scale = c(2, .25)) # Asylum Seekers' Integration in Scotland
cloud(stmodel, topic = 3, scale = c(2, .25)) # Legal Support for Asylum Seekers
cloud(stmodel, topic = 4, scale = c(2, .25)) # Migrant Justice and Human Rights
cloud(stmodel, topic = 5, scale = c(2, .25)) # Migrant Experiences and Integration Challenges
cloud(stmodel, topic = 6, scale = c(2, .25)) # Asylum Support Networks and Social Services
cloud(stmodel, topic = 7, scale = c(2, .25)) # Child Trafficking in Asylum Seeking
cloud(stmodel, topic = 8, scale = c(2, .25)) # Refugee Week and Fundraising Campaigns
cloud(stmodel, topic = 9, scale = c(2, .25)) # Decolonial Dialogues and Cognitive Justice 
cloud(stmodel, topic = 10, scale = c(2, .25)) # LGBTQI+ Experiences in Asylum Seeking
cloud(stmodel, topic = 11, scale = c(2, .25)) # Women Experiences in Asylum Seeking
cloud(stmodel, topic = 12, scale = c(2, .25)) # Asylum Accommodation and Essential Support
cloud(stmodel, topic = 13, scale = c(2, .25)) # Migrant Data and Legal Status
cloud(stmodel, topic = 14, scale = c(2, .25)) # Youth Support for Asylum Seekers and Refugees
cloud(stmodel, topic = 15, scale = c(2, .25)) # Illegal Migration Bill
```

```{r}
# Examine the documents containing a specific topic, for example topic 1
topic_1 <- findThoughts(stmodel,
                        texts = articles$Text[rowSums(dfm)>0],
                        n = 2, topics = 1)$docs[[1]]
plotQuote(topic_1, width = 300,
          main = "Documents containing topic 1")
```
### Topic Proportions
```{r}
# Convert the theta matrix to a dataframe and reshape it to long format
topic_probabilities <- as.data.frame(stmodel$theta)
topic_probabilities$document <- rownames(topic_probabilities)
doc_topic_long <- pivot_longer(topic_probabilities, -document, names_to = "topic", values_to = "prob")

# Calculate average probabilities per topic
average_probabilities <- doc_topic_long %>%
  group_by(topic) %>%
  summarize(AverageProb = mean(prob), .groups = 'drop')

# Convert topic to a factor with explicit levels
average_probabilities$topic <- factor(average_probabilities$topic, levels = paste0("V", 1:15))

# Create simple labels for the pie slices with just topic numbers and proportions
average_probabilities <- average_probabilities %>%
  mutate(Label = sprintf("%s: %.1f%%", topic, AverageProb * 100))

# Define detailed topic descriptions for the legend including topic numbers
detailed_topic_descriptions <- paste("Topic", 1:15, ":",
  c("Migrants Diversity and Historical Acknowledgement", "Asylum Seekers' Integration in Scotland",
    "Legal Support for Asylum Seekers", "Migrant Justice and Human Rights", "Migrant Experiences and Integration Challenges",
    "Asylum Support Networks and Social Services", "Child Trafficking in Asylum Seeking", "Refugee Week and Fundraising Campaigns",
    "Decolonial Dialogues and Cognitive Justice", "LGBTQI+ Experiences in Asylum Seeking", "Women Experiences in Asylum Seeking",
    "Asylum Accommodation and Essential Support", "Migrant Data and Legal Status", "Youth Support for Asylum Seekers and Refugees",
    "Illegal Migration Bill"))

# Define a custom color palette with distinct colors
custom_colors <- colorRampPalette(c("#e6194b", "#3cb44b", "#ffe119", "#4363d8", "#f58231", "#911eb4", "#46f0f0", "#f032e6", "#bcf60c", "#fabebe", "#008080", "#e6beff", "#9a6324", "#fffac8", "#800000"))(15)

# Plotting the pie chart
pie_chart <- ggplot(average_probabilities, aes(x = "", y = AverageProb, fill = topic)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  geom_label(aes(label = Label), position = position_stack(vjust = 0.5), size = 3, fontface = "bold") +
  scale_fill_manual(values = custom_colors, labels = detailed_topic_descriptions) +
  theme_void() +
  theme(legend.title = element_blank(), legend.text = element_text(size = 10), legend.position = "right") +
  guides(fill = guide_legend(title = "Topic Names"))

# Display the plot
print(pie_chart)

ggsave("topic prevalence.png", pie_chart, dpi = 300, width = 12, height = 8, units = "in")

```
### Topic Prevalence by time
```{r}
# Estimate the effect of day_of_year on topic prevalence
effect_estimates <- estimateEffect(1:15 ~ s(day_of_year), stmodel, meta = stm_input$meta)

# Detailed topic labels, include topic numbers for clarity
topic_labels <- c(
  "Migrants Diversity and\nHistorical Acknowledgement",
  "Asylum Seekers' Integration\nin Scotland",
  "Legal Support for\nAsylum Seekers",
  "Migrant Justice and\nHuman Rights",
  "Migrant Experiences and\nIntegration Challenges",
  "Asylum Support Networks\nand Social Services",
  "Child Trafficking\nin Asylum Seeking",
  "Refugee Week and\nFundraising Campaigns",
  "Decolonial Dialogues\nand Cognitive Justice",
  "LGBTQI+ Experiences\nin Asylum Seeking",
  "Women Experiences\nin Asylum Seeking",
  "Asylum Accommodation\nand Essential Support",
  "Migrant Data and\nLegal Status",
  "Youth Support for Asylum\nSeekers and Refugees",
  "Illegal Migration Bill"
)
# Save the plot
png("Topic Prevalence across Time.png", width = 3000, height = 2000, res = 300)
# Set up the layout for the plots
par(mfrow = c(3, 5), mar = c(5, 4, 2, 1), cex.axis = 0.8, cex.main = 1)

# Loop through topics to create sub-plots with correct axis labels
for (i in 1:length(topic_labels)) {
  plot(effect_estimates, "day_of_year", method = "continuous", topics = i,
       model = stmodel, printlegend = FALSE, xaxt = "n", xlab = "",  # xaxt = "n" to suppress default x-axis
       main = topic_labels[i])  # Ensure plot has the desired title
  
  # Correctly plot the x-axis labels with desired alignment
  monthseq <- seq(from = as.Date("2022-12-01"), to = as.Date("2023-12-31"), by = "month")
  monthpositions <- as.numeric(monthseq - min(monthseq))
  axis(1, at = monthpositions, labels = months(monthseq), las = 2)  # las = 2 for vertical labels
}
dev.off()

```

## Sentiment Analysis
### Word Embeddings and Sentiment Lexicon
Train the word2vec model:
```{r}
# Train the word2vec model on my corpus
set.seed(123)

tokens <- corp %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,
         remove_url = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(stopwords("en"))

# Unlist the tokens to create a single vector of words for word2vec
word_list <- unlist(tokens)
# Train the model
word2vec_model <- word2vec(x = word_list, type = "skip-gram", dim = 100, window = 10, min_count = 5, iter = 20)

embedding_matrix <- as.matrix(word2vec_model)
head(embedding_matrix[,1:10])

# Predict the nearest words to a given word
predict(word2vec_model, newdata = c("rwanda"), type = "nearest", top_n = 5)
```

Apply the dictionary as comparison, calculate the sentiment and emotion scores, and normalize the scores.
```{r}
# Calculate the sentiment scores based on the NRC lexicon
dfm_weighted <- dfm_weight(dfm, scheme = "prop")

# Apply the sentiment dictionary
dfm_sentiments <- dfm_lookup(dfm_weighted, dictionary = data_dictionary_NRC)

# Normalize the sentiment and emotion scores for the entire dfm
total_emotions <- colSums(dfm_sentiments[, c("joy", "sadness", "anger", "fear", "surprise", "disgust", "trust", "anticipation")])
total_sentiments <- colSums(dfm_sentiments[, c("positive", "negative")])

normalized_emotions <- total_emotions / sum(total_emotions)
normalized_sentiments <- total_sentiments / sum(total_sentiments)

normalized_scores <- c(normalized_emotions, normalized_sentiments)
normalized_scores_df <- as.data.frame(t(normalized_scores))
colnames(normalized_scores_df) <- c("happy", "sad", "angry", "fear", "surprise", "disgust", "trust", "anticipation", "positive", "negative")

# View the normalized scores
print(normalized_scores_df)
```

```{r}
# Convert DFM to a matrix, then to a data frame
emotion_matrix <- as.matrix(dfm_sentiments)
emotion_df <- as.data.frame(emotion_matrix)
row.names(emotion_df) <- docnames(dfm_sentiments)
emotion_df$document <- row.names(emotion_df)

# Convert row names to a column for document identifiers
emotion_long <- pivot_longer(emotion_df, cols = -document, names_to = "emotion", values_to = "sentiment_score")

# Merge with metadata
doc_metadata <- data.frame(
  document = docnames(dfm_ungrouped),
  date = as.Date(docvars(dfm_ungrouped, "date"), format = "%Y-%m-%d")
)
emotion_long <- left_join(emotion_long, doc_metadata, by = "document")

# Filter to include only relevant emotions and sentiments
emotion_long <- emotion_long %>%
  filter(emotion %in% c("joy", "trust", "fear", "surprise", "anger", "anticipation", "disgust", "sadness", "positive", "negative"))

# Normalize the sentiment and emotion scores for each document
emotion_long <- emotion_long %>%
  group_by(document) %>%
  mutate(total_emotions = sum(sentiment_score[emotion %in% c("joy", "trust", "fear", "surprise", "anger", "anticipation", "disgust", "sadness")]),
         total_sentiments = sum(sentiment_score[emotion %in% c("positive", "negative")]),
         normalized_score = if_else(emotion %in% c("positive", "negative"), sentiment_score / total_sentiments, sentiment_score / total_emotions)) %>%
  ungroup()

# Separate emotions and sentiments
emotion_long <- emotion_long %>%
  filter(!is.na(normalized_score))

emotion_data <- emotion_long %>%
  filter(emotion %in% c("joy", "trust", "fear", "surprise", "anger", "anticipation", "disgust", "sadness"))

sentiment_data <- emotion_long %>%
  filter(emotion %in% c("positive", "negative"))

# Plotting emotions over time
plot_emotions <- ggplot(emotion_data, aes(x = date, y = normalized_score, color = emotion)) +
  geom_line() +
  labs(title = "Normalized Emotions Over Time", x = "Date", y = "Normalized Score") +
  scale_color_brewer(palette = "Set3") +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month")

# Plotting sentiments over time
plot_sentiments <- ggplot(sentiment_data, aes(x = date, y = normalized_score, color = emotion)) +
  geom_line() +
  labs(title = "Normalized Sentiments Over Time", x = "Date", y = "Normalized Score") +
  scale_color_brewer(palette = "Set1") +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month")

# Combine the plots
combined_plot <- plot_emotions + plot_sentiments + plot_layout(ncol = 1)

# Print the combined plot
print(combined_plot)
```

Incorporate the word embeddings into sentiment analysis.
```{r}
# Extract the vocabulary from the word2vec model
vocabulary_summary <- summary(word2vec_model, type = "vocabulary")

word_matrix <- as.matrix(word2vec_model)
# Set up word vectors
word_vectors <- rownames(word_matrix)

# Get sentiment and emotion vectors: for each dictionary category, identify valid words in the model, extract and average the vectors
sentiment_vectors <- lapply(data_dictionary_NRC, function(category_words) {
  words_list <- unlist(category_words, use.names = FALSE)
  valid_words <- words_list[words_list %in% word_vectors]

  if (length(valid_words) > 0) {
    # Retrieve vectors for valid words
    word_indices <- match(valid_words, word_vectors)
    word_vectors_subset <- word_matrix[word_indices, , drop = FALSE]
    # Calculate the mean vector for these words
    return(colMeans(word_vectors_subset, na.rm = TRUE))
  } else {
    # Return zero vector if no valid words found
    return(rep(0, ncol(word_matrix)))
  }
})

# Prepare document strings for embedding
document_strings <- sapply(tokens, function(doc_tokens) {
  paste(doc_tokens, collapse=" ")
})

DEFAULT_DIMENSION <- 100

# Compute document embeddings with robust error handling
document_embeddings <- lapply(document_strings, function(doc_string) {
  tryCatch({
    doc_vec <- doc2vec(object = word2vec_model, newdata = doc_string, split = " ")
    if (is.null(doc_vec) || length(doc_vec) != DEFAULT_DIMENSION) {
      rep(NA_real_, DEFAULT_DIMENSION)
    } else {
      doc_vec
    }
  }, error = function(e) {
    rep(NA_real_, DEFAULT_DIMENSION)
  })
})

# Function to calculate cosine similarity
cosine_similarity <- function(vec1, vec2) {
  sum(vec1 * vec2) / (sqrt(sum(vec1^2) * sum(vec2^2)))
}
# Calculate the corpus sentiment scores
valid_embeddings <- Filter(function(v) !any(is.na(v)), document_embeddings)
if (length(valid_embeddings) > 0) {
    corpus_embedding <- Reduce("+", valid_embeddings) / length(valid_embeddings)
} else {
    corpus_embedding <- rep(0, DEFAULT_DIMENSION)  # Fallback if no valid embeddings
}

# Calculate cosine similarity scores with each sentiment and emotion vector
corpus_sentiment_scores <- sapply(sentiment_vectors, function(sent_vec) {
    cosine_similarity(corpus_embedding, sent_vec)
})

names(corpus_sentiment_scores) <- c("positive", "negative", "joy", "sadness", "anger", "fear", "surprise", "disgust", "trust", "anticipation")

# Separate sentiments and emotions for normalization
sentiment_scores <- corpus_sentiment_scores[c("positive", "negative")]
emotion_scores <- corpus_sentiment_scores[c("joy", "sadness", "anger", "fear", "surprise", "disgust", "trust", "anticipation")]

# Normalize sentiment scores
if (sum(sentiment_scores) != 0) {
  normalized_sentiment_scores <- sentiment_scores / sum(sentiment_scores)
} else {
  normalized_sentiment_scores <- sentiment_scores
}

# Normalize emotion scores
if (sum(emotion_scores) != 0) {
  normalized_emotion_scores <- emotion_scores / sum(emotion_scores)
} else {
  normalized_emotion_scores <- emotion_scores
}

# Combine normalized scores
normalized_scores <- c(normalized_sentiment_scores, normalized_emotion_scores)
names(normalized_scores) <- names(corpus_sentiment_scores)  # Ensure names are consistent

# Print normalized scores
print("Normalized Corpus-Wide Sentiment and Emotion Scores:")
print(normalized_scores)
```
Visualise the sentiment and emotion scores for the overall corpus.
```{r}
# Create a data frame for the scores
scores_df <- data.frame(
  Category = names(normalized_scores),
  Score = normalized_scores
)
# Enhance the data frame with a 'Type' column
scores_df$Type <- ifelse(scores_df$Category %in% c("positive", "negative"), "Sentiments", "Emotions")

# Print the data frame to check its contents
print(scores_df)

# Generate the faceted bar chart with labels
ggplot(scores_df, aes(x = Category, y = Score, fill = Category)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = sprintf("%.2f", Score)), position = position_stack(vjust = 0.5), color = "white") + 
  facet_wrap(~ Type, scales = "free_x") +
  theme_minimal() +
  labs(title = "Normalised Sentiment and Emotion Scores",
       x = "Category",
       y = "Normalised Score") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") +
  scale_fill_manual(values = c("positive" = "#00AFBB", "negative" = "#FC4E07",
                               "joy" = "#E7B800", "sadness" = "#6C8EBF", "anger" = "#D62728",
                               "fear" = "#9467BD", "surprise" = "#8C564B", "disgust" = "#E377C2",
                               "trust" = "#1F77B4", "anticipation" = "#2CA02C"))

```
Visualise across time:
```{r}
# Map document embeddings to their sentiment scores
document_sentiment_scores <- lapply(document_embeddings, function(doc_embedding) {
  sapply(sentiment_vectors, function(sent_vec) {
    cosine_similarity(doc_embedding, sent_vec)
  })
})

# Convert list to dataframe
scores_df <- do.call(rbind, document_sentiment_scores)
colnames(scores_df) <- names(sentiment_vectors)  # Ensure column names are set from sentiment vector names

# Convert scores_df from matrix to dataframe
scores_df <- as.data.frame(scores_df)

# Normalize sentiment and emotion scores within each document
scores_df <- scores_df %>%
  mutate(
    total_sentiment = rowSums(select(., positive, negative)),
    total_emotion = rowSums(select(., joy, sadness, anger, fear, surprise, disgust, trust, anticipation))
  ) %>%
  mutate(
    across(c("positive", "negative"), ~ ./total_sentiment),
    across(c("joy", "sadness", "anger", "fear", "surprise", "disgust", "trust", "anticipation"), ~ ./total_emotion)
  )

# Add document date to the scores dataframe
scores_df$date <- as.Date(dfm$date, format = "%d-%b-%y")

scores_long <- pivot_longer(scores_df, cols = -date, names_to = "category", values_to = "score")

# Create a flag to distinguish between sentiment and emotion categories
scores_long <- scores_long %>%
  mutate(type = case_when(
    category %in% c("positive", "negative") ~ "sentiment",
    TRUE ~ "emotion"
  ))

# Aggregate scores by day and category
daily_scores <- scores_long %>%
  group_by(date, category, type) %>%
  summarise(daily_score = sum(score), .groups = "drop")

# Filter out the aggregate total columns before normalizing
filtered_scores <- daily_scores %>%
  filter(category %in% c("positive", "negative", "joy", "sadness", "anger", "fear", "surprise", "disgust", "trust", "anticipation"))

# Normalize the daily scores for emotions and sentiments separately
normalized_daily_scores <- filtered_scores %>%
  group_by(date, type) %>%
  mutate(total_score = sum(daily_score)) %>%
  ungroup() %>%
  mutate(normalized_score = daily_score / total_score)

# Separate sentiments and emotions for plotting
sentiments <- normalized_daily_scores %>%
  filter(type == "sentiment")

emotions <- normalized_daily_scores %>%
  filter(type == "emotion")

# Plotting sentiments
p1 <- ggplot(sentiments, aes(x = date, y = normalized_score, color = category)) +
  geom_line() +
  labs(title = "Normalised Daily Sentiment Scores", y = "Normalised Score", x = "Date") +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month")

# Plotting emotions
p2 <- ggplot(emotions, aes(x = date, y = normalized_score, color = category)) +
  geom_line() +
  labs(title = "Normalised Daily Emotion Scores", y = "Normalised Score", x = "Date") +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month")

# Combine the plots
combined_plot <- p1 / p2 + plot_layout(ncol = 1)
print(combined_plot)

# Save the combined plot to a file
ggsave("combined_sentiment_emotion_plot.png", plot = combined_plot, width = 10, height = 8, dpi = 300)
```
Match topics with sentiment and emotion scores, so to know the sentiment and emotion scores for each topic across time.
```{r}
# Extract the dominant topic for each document
dominant_topic_per_doc <- doc_topic_long %>%
  group_by(document) %>%
  summarise(dominant_topic = topic[which.max(prob)], .groups = 'drop')

# Add the prefix 'text' to the document identifiers in dominant_topic_per_doc for merging
dominant_topic_per_doc$document <- paste("text", dominant_topic_per_doc$document, sep="")
# Add the column 'document' to scores_df for merging
scores_df$document <- rownames(scores_df)

# Merge dominant topic per document with the sentiment scores
document_sentiment_topic <- merge(scores_df, dominant_topic_per_doc, by = "document")

# Aggregate sentiment scores by topic and date
topic_date_sentiments <- document_sentiment_topic %>%
  group_by(date, dominant_topic) %>%
  summarise_at(vars(positive, negative, joy, sadness, anger, fear, surprise, disgust, trust, anticipation), mean, .groups = 'drop')

# Custom topic labels mapping from V1 to V15
topic_labels <- c(
  "V1" = "Migrants Diversity and\nHistorical Acknowledgement",
  "V2" = "Asylum Seekers' Integration in Scotland",
  "V3" = "Legal Support for Asylum Seekers",
  "V4" = "Migrant Justice and Human Rights",
  "V5" = "Migrant Experiences and\nIntegration Challenges",
  "V6" = "Asylum Support Networks\nand Social Services",
  "V7" = "Child Trafficking in Asylum Seeking",
  "V8" = "Refugee Week and Fundraising Campaigns",
  "V9" = "Decolonial Dialogues and Cognitive Justice",
  "V10" = "LGBTQI+ Experiences in Asylum Seeking",
  "V11" = "Women Experiences in Asylum Seeking",
  "V12" = "Asylum Accommodation\nand Essential Support",
  "V13" = "Migrant Data and Legal Status",
  "V14" = "Youth Support for Asylum\nSeekers and Refugees",
  "V15" = "Illegal Migration Bill"
)

# Split the data into two subsets: one for sentiments and one for emotions for clearer plotting
sentiments_data <- topic_date_sentiments %>%
  select(date, dominant_topic, positive, negative)

emotions_data <- topic_date_sentiments %>%
  select(date, dominant_topic, joy, sadness, anger, fear, surprise, disgust, trust, anticipation)

# Sentiment Plot
sentiment_plot <- ggplot(data = sentiments_data, aes(x = date)) + 
  geom_line(aes(y = positive, color = "Positive")) +
  geom_line(aes(y = negative, color = "Negative")) +
  facet_wrap(~ dominant_topic, labeller = labeller(dominant_topic = topic_labels), scales = "free_y") +
  scale_x_date(date_labels = "%b %Y", date_breaks = "2 months") +  # x axis is divided by every two months
  labs(title = "Sentiment Scores Over Time by Topic", y = "Sentiment Score", x = "Date") +
  scale_color_manual(values = c("Positive" = "#00AFBB", "Negative" = "#FC4E07")) +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1))  # Slight rotation for better readability

# Emotion Plot
emotion_plot <- ggplot(data = emotions_data, aes(x = date)) + 
  geom_line(aes(y = joy, color = "Joy")) +
  geom_line(aes(y = sadness, color = "Sadness")) +
  geom_line(aes(y = anger, color = "Anger")) +
  geom_line(aes(y = fear, color = "Fear")) +
  geom_line(aes(y = surprise, color = "Surprise")) +
  geom_line(aes(y = disgust, color = "Disgust")) +
  geom_line(aes(y = trust, color = "Trust")) +
  geom_line(aes(y = anticipation, color = "Anticipation")) +
  facet_wrap(~ dominant_topic, labeller = labeller(dominant_topic = topic_labels), scales = "free_y") +
  scale_x_date(date_labels = "%b %Y", date_breaks = "2 months") +  # x axis is divided by every two months
  labs(title = "Emotion Scores Over Time by Topic", y = "Emotion Score", x = "Date") +
  scale_color_manual(values = c("Joy" = "#00AFBB", "Sadness" = "#006994", "Anger" = "#FC4E07", "Fear" = "#008000", 
                                "Surprise" = "#9467BD", "Disgust" = "#9ACD32", "Trust" = "#E377C2", "Anticipation" = "#E7B800")) +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank(),
        axis.text.x = element_text(angle = 45, hjust = 1))  # Slight rotation for better readability

# Print the plots
print(sentiment_plot)
print(emotion_plot)

# Save the plots
ggsave("sentiment_scores_by_topic.png", plot = sentiment_plot, width = 12, height = 8, dpi = 300)
ggsave("emotion_scores_by_topic.png", plot = emotion_plot, width = 12, height = 8, dpi = 300)
```

Migrant events timeline to match the structural topic model and sentiment scores:
```{r}
# Load and prepare data
events <- read.csv("migrant events timeline.csv", stringsAsFactors = FALSE)
events$Date <- as.Date(events$Date, format = "%d/%m/%Y")

# Function to add line breaks to event text
split_text <- function(text, width) {
  str_wrap(text, width = width)
}

# Apply text splitting function to events
events <- events %>%
  mutate(
    Event = sapply(Event, split_text, width = 30)  # Adjust width as necessary
  )

# Assign initial staggered positions based on each event's order within its month to avoid overlap
events <- events %>%
  arrange(Date) %>%
  group_by(Month = format(Date, "%Y-%m")) %>%
  mutate(
    Position = 1.0 * (row_number() - 0.5) * ifelse(row_number() %% 2 == 0, 1, -1),  # Initial staggered positions
    Vjust = ifelse(Position > 0, -0.5, 1.5)  # Calculate vjust based on Position
  ) %>%
  ungroup()

# Function to check for overlaps and adjust positions
adjust_positions <- function(df) {
  max_iter <- 10  # Maximum number of iterations to prevent infinite loops
  for (i in 2:nrow(df)) {
    iter_count <- 0
    repeat {
      overlap_found <- FALSE
      for (j in 1:(i - 1)) {
        if (abs(df$Position[i] - df$Position[j]) < 1 && abs(difftime(df$Date[i], df$Date[j], units = "days")) < 15) {
          df$Position[i] <- df$Position[i] + ifelse(df$Position[i] > 0, 1, -1) * 0.5
          overlap_found <- TRUE
        }
      }
      if (!overlap_found || iter_count >= max_iter) {
        break
      }
      iter_count <- iter_count + 1
    }
  }
  return(df)
}

events <- adjust_positions(events)

# Create the timeline plot
timeline_plot <- ggplot(events, aes(x = Date)) +
  geom_hline(yintercept = 0, color = "black", size = 1.5) +  # Horizontal base line
  geom_segment(aes(y = 0, xend = Date, yend = Position), color = "black", size = 0.5) +  # Vertical indicator lines
  geom_text(aes(y = Position, label = Event, vjust = Vjust), hjust = 0.5, size = 3.3, color = "black") +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +  # Month labels directly on the timeline
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),  # Adjusted angle of month labels to prevent overlap
    axis.ticks.x = element_blank(),
    axis.line.x = element_line(color = "black", size = 0.5),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.background = element_rect(fill = "white"),
    plot.margin = margin(t = 10, r = 10, b = 10, l = 10, unit = "pt")  # Reduced bottom margin
  ) +
  labs(x = "", y = "", title = "Timeline of UK Immigration Events")

# Display the plot
print(timeline_plot)

# Save the plot as an image
ggsave("Timeline_Plot.png", plot = timeline_plot, width = 22, height = 16, dpi = 320)

# Read the image and rotate it
image <- image_read("Timeline_Plot.png")
rotated_image <- image_rotate(image, 270)

# Save the rotated image as a PDF
pdf("Timeline_Plot_A4_Rotated.pdf", width = 8.27, height = 11.69)
grid::grid.raster(rotated_image)
dev.off()
```